{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex  Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male   22      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female   38      1   \n",
      "2                             Heikkinen, Miss. Laina  female   26      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female   35      1   \n",
      "4                           Allen, Mr. William Henry    male   35      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD   # Stochastic Gradient Descent\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import sklearn.datasets as datasets\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline\n",
    "\n",
    "titanic = pd.read_csv(\"/Users/GCleverly/Documents/Datascienceprojects/data_titanic/train.csv\")\n",
    "\n",
    "# Print the first 5 rows of the dataframe.\n",
    "print(titanic.head(5))\n",
    "\n",
    "print(titanic.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The titanic variable is available here.\n",
    "titanic['Age'] = titanic['Age'].fillna(titanic['Age'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male' 'female']\n"
     ]
    }
   ],
   "source": [
    "# Find all the unique genders -- the column appears to contain only male and female.\n",
    "print(titanic[\"Sex\"].unique())\n",
    "\n",
    "# Replace all the occurences of male with the number 0.\n",
    "titanic.loc[titanic[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "titanic.loc[titanic[\"Sex\"] == \"female\",\"Sex\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S' 'C' 'Q' nan]\n"
     ]
    }
   ],
   "source": [
    "# Find all the unique values for \"Embarked\".\n",
    "print(titanic[\"Embarked\"].unique())\n",
    "\n",
    "titanic.loc[titanic['Embarked'].isnull(),'Embarked'] = \"S\"\n",
    "titanic.loc[titanic['Embarked'] == \"S\",'Embarked'] = 0\n",
    "titanic.loc[titanic['Embarked'] == \"C\",'Embarked'] = 1\n",
    "titanic.loc[titanic['Embarked'] == \"Q\",'Embarked'] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The columns we'll use to predict the target\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm class\n",
    "alg = LinearRegression()\n",
    "# Generate cross validation folds for the titanic dataset.  It return the row indices corresponding to train and test.\n",
    "# We set random_state to ensure we get the same splits every time we run this.\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    # The predictors we're using the train the algorithm.  Note how we only take the rows in the train folds.\n",
    "    train_predictors = (titanic[predictors].iloc[train,:])\n",
    "    # The target we're using to train the algorithm.\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    # Training the algorithm using the predictors and target.\n",
    "    alg.fit(train_predictors, train_target)\n",
    "    # We can now make predictions on the test fold\n",
    "    test_predictions = alg.predict(titanic[predictors].iloc[test,:])\n",
    "    predictions.append(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7833894500561167\n"
     ]
    }
   ],
   "source": [
    "# The predictions are in three separate numpy arrays.  Concatenate them into one.  \n",
    "# We concatenate them on axis 0, as they only have one axis.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Map predictions to outcomes (only possible outcomes are 1 and 0)\n",
    "predictions[predictions > .5] = 1\n",
    "predictions[predictions <=.5] = 0\n",
    "\n",
    "num_correct = (titanic[\"Survived\"] == predictions)\n",
    "num_correct = num_correct[num_correct == True]\n",
    "accuracy = len(num_correct)/len(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.787878787879\n"
     ]
    }
   ],
   "source": [
    "# Initialize our algorithm\n",
    "alg = LogisticRegression(random_state=1)\n",
    "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic_test = pd.read_csv(\"/Users/GCleverly/Documents/GA-DS-15/data_titanic/test.csv\")\n",
    "\n",
    "titanic_test['Age'] = titanic_test['Age'].fillna(titanic['Age'].median())\n",
    "titanic_test['Fare'] = titanic_test['Fare'].fillna(titanic_test['Fare'].median())\n",
    "\n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "\n",
    "titanic_test.loc[titanic_test[\"Embarked\"].isnull(), \"Embarked\"] = \"S\"\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"Q\", \"Embarked\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the algorithm class\n",
    "alg = LogisticRegression(random_state=1)\n",
    "\n",
    "# Train the algorithm using all the training data\n",
    "alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Make predictions using the test set.\n",
    "predictions = alg.predict(titanic_test[predictors])\n",
    "\n",
    "# Create a new dataframe with only the columns Kaggle wants from the dataset.\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "submission.to_csv(\"Submission_Titanic.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.801346801347\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm with the default paramters\n",
    "# n_estimators is the number of trees we want to make\n",
    "# min_samples_split is the minimum number of rows we need to make a split\n",
    "# min_samples_leaf is the minimum number of samples we can have at the place where a tree branch ends (the bottom points of the tree)\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, min_samples_leaf=1)\n",
    "\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg,titanic[predictors],titanic[\"Survived\"],cv=3)\n",
    "\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.820426487093\n"
     ]
    }
   ],
   "source": [
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=4, min_samples_leaf=2)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg,titanic[predictors],titanic[\"Survived\"],cv=3)\n",
    "\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating a familysize column\n",
    "titanic[\"FamilySize\"] = titanic[\"SibSp\"] + titanic[\"Parch\"]\n",
    "\n",
    "# The .apply method generates a new series\n",
    "titanic[\"NameLength\"] = titanic[\"Name\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr          517\n",
      "Miss        182\n",
      "Mrs         125\n",
      "Master       40\n",
      "Dr            7\n",
      "Rev           6\n",
      "Col           2\n",
      "Mlle          2\n",
      "Major         2\n",
      "Lady          1\n",
      "Sir           1\n",
      "Don           1\n",
      "Ms            1\n",
      "Countess      1\n",
      "Mme           1\n",
      "Capt          1\n",
      "Jonkheer      1\n",
      "Name: Name, dtype: int64\n",
      "1     517\n",
      "2     183\n",
      "3     125\n",
      "4      40\n",
      "5       7\n",
      "6       6\n",
      "7       5\n",
      "10      3\n",
      "8       3\n",
      "9       2\n",
      "Name: Name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# A function to get the title from a name.\n",
    "def get_title(name):\n",
    "    # Use a regular expression to search for a title.  Titles always consist of capital and lowercase letters, and end with a period.\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "# Get all the titles and print how often each one occurs.\n",
    "titles = titanic[\"Name\"].apply(get_title)\n",
    "print(pd.value_counts(titles))\n",
    "\n",
    "# Map each title to an integer.  Some titles are very rare, and are compressed into the same codes as other titles.\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "\n",
    "# Verify that we converted everything.\n",
    "print(pd.value_counts(titles))\n",
    "\n",
    "# Add in the title column.\n",
    "titanic[\"Title\"] = titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1      800\n",
      " 14       8\n",
      " 149      7\n",
      " 63       6\n",
      " 50       6\n",
      " 59       6\n",
      " 17       5\n",
      " 384      4\n",
      " 27       4\n",
      " 25       4\n",
      " 162      4\n",
      " 8        4\n",
      " 84       4\n",
      " 340      4\n",
      " 43       3\n",
      " 269      3\n",
      " 58       3\n",
      " 633      2\n",
      " 167      2\n",
      " 280      2\n",
      " 510      2\n",
      " 90       2\n",
      " 83       1\n",
      " 625      1\n",
      " 376      1\n",
      " 449      1\n",
      " 498      1\n",
      " 588      1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# A dictionary mapping family name to id\n",
    "family_id_mapping = {}\n",
    "\n",
    "# A function to get the id given a row\n",
    "def get_family_id(row):\n",
    "    # Find the last name by splitting on a comma\n",
    "    last_name = row[\"Name\"].split(\",\")[0]\n",
    "    # Create the family id\n",
    "    family_id = \"{0}{1}\".format(last_name, row[\"FamilySize\"])\n",
    "    # Look up the id in the mapping\n",
    "    if family_id not in family_id_mapping:\n",
    "        if len(family_id_mapping) == 0:\n",
    "            current_id = 1\n",
    "        else:\n",
    "            # Get the maximum id from the mapping and add one to it if we don't have an id\n",
    "            current_id = (max(family_id_mapping.items(), key=operator.itemgetter(1))[1] + 1)\n",
    "        family_id_mapping[family_id] = current_id\n",
    "    return family_id_mapping[family_id]\n",
    "\n",
    "# Get the family ids with the apply method\n",
    "family_ids = titanic.apply(get_family_id, axis=1)\n",
    "\n",
    "# There are a lot of family ids, so we'll compress all of the families under 3 members into one code.\n",
    "family_ids[titanic[\"FamilySize\"] < 3] = -1\n",
    "\n",
    "# Print the count of each unique id.\n",
    "print(pd.value_counts(family_ids))\n",
    "\n",
    "titanic[\"FamilyId\"] = family_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAEsCAYAAAAIBeLrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11160e668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.806958473625\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform from p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "# Plot the scores.  See how \"Pclass\", \"Sex\", \"Title\", and \"Fare\" are the best?\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "# Pick only the four best features.\n",
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Title\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=4, min_samples_leaf=2)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg,titanic[predictors],titanic[\"Survived\"],cv=3)\n",
    "\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.819304152637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:33: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "# The algorithms we want to ensemble.\n",
    "# We're using the more linear predictors for the logistic regression, and everything with the gradient boosting classifier.\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "# Initialize the cross validation folds\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    full_test_predictions = []\n",
    "    # Make predictions for each algorithm on each fold\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm on the training data.\n",
    "        alg.fit(titanic[predictors].iloc[train,:], train_target)\n",
    "        # Select and predict on the test fold.  \n",
    "        # The .astype(float) is necessary to convert the dataframe to all floats and avoid an sklearn error.\n",
    "        test_predictions = alg.predict_proba(titanic[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_predictions.append(test_predictions)\n",
    "    # Use a simple ensembling scheme -- just average the predictions to get the final classification.\n",
    "    test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
    "    # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction.\n",
    "    test_predictions[test_predictions <= .5] = 0\n",
    "    test_predictions[test_predictions > .5] = 1\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "# Put all the predictions together into one array.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Compute accuracy by comparing to the training data.\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     240\n",
      "2      79\n",
      "3      72\n",
      "4      21\n",
      "7       2\n",
      "6       2\n",
      "10      1\n",
      "5       1\n",
      "Name: Title, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# First, we'll add titles to the test set.\n",
    "titles = titanic_test[\"Name\"].apply(get_title)\n",
    "# We're adding the Dona title to the mapping, because it's in the test set, but not the training set\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2, \"Dona\": 10}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "titanic_test[\"Title\"] = titles\n",
    "# Check the counts of each unique title.\n",
    "print(pd.value_counts(titanic_test[\"Title\"]))\n",
    "\n",
    "# Now, we add the family size column.\n",
    "titanic_test[\"FamilySize\"] = titanic_test[\"SibSp\"] + titanic_test[\"Parch\"]\n",
    "\n",
    "# Now we can add family ids.\n",
    "# We'll use the same ids that we did earlier.\n",
    "#print(family_id_mapping)\n",
    "\n",
    "family_ids = titanic_test.apply(get_family_id, axis=1)\n",
    "family_ids[titanic_test[\"FamilySize\"] < 3] = -1\n",
    "titanic_test[\"FamilyId\"] = family_ids\n",
    "\n",
    "titanic_test[\"NameLength\"] = titanic_test[\"Name\"].apply(lambda x: len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "full_predictions = []\n",
    "for alg, predictors in algorithms:\n",
    "    # Fit the algorithm using the full training data.\n",
    "    alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "    # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error.\n",
    "    predictions = alg.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
    "    full_predictions.append(predictions)\n",
    "\n",
    "# The gradient boosting classifier generates better predictions, so we weight it higher.\n",
    "predictions = (full_predictions[0] * 3 + full_predictions[1]) / 4\n",
    "\n",
    "predictions[predictions > 0.5] = 1\n",
    "predictions[predictions <= 0.5] = 0\n",
    "predictions = predictions.astype(int)\n",
    "\n",
    "#print(len(predictions))\n",
    "#print(len(titanic[\"PassengerId\"]))\n",
    "   \n",
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\":titanic_test[\"PassengerId\"],\n",
    "    \"Survived\":predictions \n",
    "    })\n",
    "submission.to_csv(\"Submission_Titanic.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan 'C85' 'C123' 'E46' 'G6' 'C103' 'D56' 'A6' 'C23 C25 C27' 'B78' 'D33'\n",
      " 'B30' 'C52' 'B28' 'C83' 'F33' 'F G73' 'E31' 'A5' 'D10 D12' 'D26' 'C110'\n",
      " 'B58 B60' 'E101' 'F E69' 'D47' 'B86' 'F2' 'C2' 'E33' 'B19' 'A7' 'C49' 'F4'\n",
      " 'A32' 'B4' 'B80' 'A31' 'D36' 'D15' 'C93' 'C78' 'D35' 'C87' 'B77' 'E67'\n",
      " 'B94' 'C125' 'C99' 'C118' 'D7' 'A19' 'B49' 'D' 'C22 C26' 'C106' 'C65'\n",
      " 'E36' 'C54' 'B57 B59 B63 B66' 'C7' 'E34' 'C32' 'B18' 'C124' 'C91' 'E40'\n",
      " 'T' 'C128' 'D37' 'B35' 'E50' 'C82' 'B96 B98' 'E10' 'E44' 'A34' 'C104'\n",
      " 'C111' 'C92' 'E38' 'D21' 'E12' 'E63' 'A14' 'B37' 'C30' 'D20' 'B79' 'E25'\n",
      " 'D46' 'B73' 'C95' 'B38' 'B39' 'B22' 'C86' 'C70' 'A16' 'C101' 'C68' 'A10'\n",
      " 'E68' 'B41' 'A20' 'D19' 'D50' 'D9' 'A23' 'B50' 'A26' 'D48' 'E58' 'C126'\n",
      " 'B71' 'B51 B53 B55' 'D49' 'B5' 'B20' 'F G63' 'C62 C64' 'E24' 'C90' 'C45'\n",
      " 'E8' 'B101' 'D45' 'C46' 'D30' 'E121' 'D11' 'E77' 'F38' 'B3' 'D6' 'B82 B84'\n",
      " 'D17' 'A36' 'B102' 'B69' 'E49' 'C47' 'D28' 'E17' 'A24' 'C50' 'B42' 'C148']\n"
     ]
    }
   ],
   "source": [
    "cabins = titanic[\"Cabin\"].unique()\n",
    "print(cabins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'E', 'G', 'D', 'A', 'B', 'F', 'T']\n",
      "       Cabin  Pclass  Survived\n",
      "0    missing       3         0\n",
      "1          C       1         1\n",
      "2    missing       3         1\n",
      "3          C       1         1\n",
      "4    missing       3         0\n",
      "5    missing       3         0\n",
      "6          E       1         0\n",
      "7    missing       3         0\n",
      "8    missing       3         1\n",
      "9    missing       2         1\n",
      "10         G       3         1\n",
      "11         C       1         1\n",
      "12   missing       3         0\n",
      "13   missing       3         0\n",
      "14   missing       3         0\n",
      "15   missing       2         1\n",
      "16   missing       3         0\n",
      "17   missing       2         1\n",
      "18   missing       3         0\n",
      "19   missing       3         1\n",
      "20   missing       2         0\n",
      "21         D       2         1\n",
      "22   missing       3         1\n",
      "23         A       1         1\n",
      "24   missing       3         0\n",
      "25   missing       3         1\n",
      "26   missing       3         0\n",
      "27         C       1         0\n",
      "28   missing       3         1\n",
      "29   missing       3         0\n",
      "..       ...     ...       ...\n",
      "861  missing       2         0\n",
      "862        D       1         1\n",
      "863  missing       3         0\n",
      "864  missing       2         0\n",
      "865  missing       2         1\n",
      "866  missing       2         1\n",
      "867        A       1         0\n",
      "868  missing       3         0\n",
      "869  missing       3         1\n",
      "870  missing       3         0\n",
      "871        D       1         1\n",
      "872        B       1         0\n",
      "873  missing       3         0\n",
      "874  missing       2         1\n",
      "875  missing       3         1\n",
      "876  missing       3         0\n",
      "877  missing       3         0\n",
      "878  missing       3         0\n",
      "879        C       1         1\n",
      "880  missing       2         1\n",
      "881  missing       3         0\n",
      "882  missing       3         0\n",
      "883  missing       2         0\n",
      "884  missing       3         0\n",
      "885  missing       3         0\n",
      "886  missing       2         0\n",
      "887        B       1         1\n",
      "888  missing       3         0\n",
      "889        C       1         1\n",
      "890  missing       3         0\n",
      "\n",
      "[891 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/pandas/core/indexing.py:115: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "#Find the different cabin suffixes\n",
    "cabin_class = []\n",
    "for k in range(1,len(cabins)):\n",
    "    if cabins[k][0] not in cabin_class:\n",
    "        cabin_class.append(cabins[k][0])\n",
    "print(cabin_class)\n",
    "\n",
    "cabin_data = titanic[['Cabin','Pclass','Survived']].copy()\n",
    "\n",
    "cabin_data[\"Cabin\"] = cabin_data[\"Cabin\"].fillna(\"missing\")\n",
    "\n",
    "for i,y in zip(cabin_data[\"Cabin\"],cabin_data[\"Cabin\"].index):\n",
    "    if len(i) > 4:\n",
    "        cabin_data[\"Cabin\"].iloc[y] = i.split()[0]\n",
    "\n",
    "for b in cabin_class:\n",
    "    pattern = \"{}.$|{}[0-9].$|{}[0-9][0-9].$\".format(b,b,b)\n",
    "    cabin_data[\"Cabin\"] = cabin_data[\"Cabin\"].str.replace(pattern,b)\n",
    "print(cabin_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAEsCAYAAAA1l4ONAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLkkN+//+XvhzKSklcAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11316ca90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's add the cabin part to our training set\n",
    "titanic[\"Cabin\"] = cabin_data[\"Cabin\"]\n",
    "titanic['Survived'].groupby(titanic.Cabin).mean().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic.loc[titanic['Cabin'] == \"missing\",'Cabin'] = 0\n",
    "titanic.loc[titanic['Cabin'] == \"A\",'Cabin'] = 1\n",
    "titanic.loc[titanic['Cabin'] == \"B\",'Cabin'] = 2\n",
    "titanic.loc[titanic['Cabin'] == \"C\",'Cabin'] = 3\n",
    "titanic.loc[titanic['Cabin'] == \"D\",'Cabin'] = 4\n",
    "titanic.loc[titanic['Cabin'] == \"E\",'Cabin'] = 5\n",
    "titanic.loc[titanic['Cabin'] == \"F\",'Cabin'] = 6\n",
    "titanic.loc[titanic['Cabin'] == \"G\",'Cabin'] = 7\n",
    "titanic.loc[titanic['Cabin'] == \"T\",'Cabin'] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.828282828283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:32: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "#Let use the algorithm again with the Cabin Data\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), [\"Cabin\",\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]],\n",
    "    [LogisticRegression(random_state=1), [\"Cabin\",\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "# Initialize the cross validation folds\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    full_test_predictions = []\n",
    "    # Make predictions for each algorithm on each fold\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm on the training data.\n",
    "        alg.fit(titanic[predictors].iloc[train,:], train_target)\n",
    "        # Select and predict on the test fold.  \n",
    "        # The .astype(float) is necessary to convert the dataframe to all floats and avoid an sklearn error.\n",
    "        test_predictions = alg.predict_proba(titanic[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_predictions.append(test_predictions)\n",
    "    # Use a simple ensembling scheme -- just average the predictions to get the final classification.\n",
    "    test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
    "    # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction.\n",
    "    test_predictions[test_predictions <= .5] = 0\n",
    "    test_predictions[test_predictions > .5] = 1\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "# Put all the predictions together into one array.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Compute accuracy by comparing to the training data.\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAEsCAYAAAAIBeLrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHIlJREFUeJzt3XmcpVV95/HPt2lUQMUWpcsoymJE1KgwBjEmUop5iSYs\nEcHgMkhCzLxmFBKiAeJEWmZcYFxBjRqVtGuAICqJkRZIuQ6iLLIIrbgQnbGLYRUhKst3/jjPpS7V\nVV23uuuee0/39/161avu89S9dX613O997nnOOY9sExERbVg26gIiImJwCe2IiIYktCMiGpLQjoho\nSEI7IqIhCe2IiIYsGNqSniDpMkmXdp9vk3S0pBWS1khaK+k8SdvXKDgiYkumxYzTlrQM+CnwTOA1\nwE22T5F0HLDC9vHDKTMiImDx3SPPB35g+yfAQcDqbv9q4OClLCwiIta32NB+KfCp7vZK29MAttcB\nOy5lYRERsb6BQ1vS1sCBwFndrtn9KpkPHxExZMsXcd8XApfYvrHbnpa00va0pAnghrkeJClhHhGx\nEWxr9r7FdI8cDny6b/vzwKu620cAn9tAwyP9OPHEE0dew7jUMQ41jEsd41DDuNQxDjWMSx3jUIM9\n/7HuQKEtaVvKScjP9O0+Gfh9SWuB/YC3DfK9IiJi4w3UPWL7TuCRs/bdTAnyiIioZIuYEXnqqR9A\nUpWPiYmd561jcnKy2s88zjXAeNQxDjXAeNQxDjXAeNQxDjVsyKIm12xUA5KH3cYANVBvcIs22B8V\nETEISXgTT0RGRMSIJbQjIhqS0I6IaEhCOyKiIQntiIiGJLQjIhqS0I6IaEhCOyKiIQntiIiGJLQj\nIhqS0I6IaEhCOyKiIQntiIiGJLQjIhqS0I6IaEhCOyKiIQntiIiGJLQjIhqS0I6IaEhCOyKiIQnt\niIiGDBTakraXdJakayRdLemZklZIWiNpraTzJG0/7GIjIrZ0gx5pvwf4gu09gKcB1wLHA+fb3h24\nEDhhOCVGRESPbG/4DtJDgcts7zZr/7XAvranJU0AU7afOMfjvVAbwyYJqFWDGPXPGxHtk4Rtzd4/\nyJH2LsCNkk6XdKmkD0naFlhpexrA9jpgx6UtOSIiZhsktJcDewHvs70XcAela2T24WQOLyMihmz5\nAPf5KfAT29/uts+mhPa0pJV93SM3zPcNVq1add/tyclJJicnN7rgiIjN0dTUFFNTUwveb8E+bQBJ\nXwb+zPb3JJ0IbNt96WbbJ0s6Dlhh+/g5Hps+7YiIRZqvT3vQ0H4a8GFga+CHwJHAVsCZwE7A9cBh\ntm+d47EJ7YiIRdqk0N7EhhPaERGLtCmjRyIiYkwktCMiGpLQjohoSEI7IqIhCe2IiIYktCMiGpLQ\njohoSEI7IqIhCe2IiIYktCMiGpLQjohoSEI7IqIhCe2IiIYktCMiGpLQjohoSEI7IqIhCe2IiIYk\ntCMiGpLQjohoSEI7IqIhCe2IiIYktCMiGpLQjohoyPJB7iTpx8BtwL3AXbb3lrQCOAN4HPBj4DDb\ntw2pzoiIYPAj7XuBSdt72t6723c8cL7t3YELgROGUWBERMwYNLQ1x30PAlZ3t1cDBy9VURERMbdB\nQ9vAlyR9S9JR3b6VtqcBbK8DdhxGgRERMWOgPm3g2bZ/JumRwBpJaylB3m/2dkRELLGBQtv2z7rP\n/0/SZ4G9gWlJK21PS5oAbpjv8atWrbrv9uTkJJOTk5tSc0TEZmdqaoqpqakF7yd7wwfIkrYFltn+\nhaTtgDXAm4D9gJttnyzpOGCF7ePneLwXamPYJFHvjYAY9c8bEe2ThG2tt3+A0N4FOIeSesuBT9p+\nm6SHA2cCOwHXU4b83TrH4xPaERGLtNGhvQQNJ7QjIhZpvtDOjMiIiIYktCMiGpLQjohoSEI7IqIh\nCe2IiIYktCMiGpLQjohoSEI7IqIhCe2IiIYktCMiGpLQjohoSEI7IqIhCe2IiIYktCMiGpLQjoho\nSEI7IqIhCe2IiIYktCMiGpLQjohoSEI7IqIhCe2IiIYktCMiGpLQjohoyMChLWmZpEslfb7bXiFp\njaS1ks6TtP3wyoyICFjckfYxwHf7to8Hzre9O3AhcMJSFhYREesbKLQlPQZ4EfDhvt0HAau726uB\ng5e2tIiImG3QI+13Aa8H3Ldvpe1pANvrgB2XuLaIiJhlwdCW9AfAtO3LAW3grt7A1yIiYgksH+A+\nzwYOlPQiYBvgIZI+DqyTtNL2tKQJ4Ib5vsGqVavuuz05Ocnk5OQmFR0RsbmZmppiampqwfvJHvwA\nWdK+wF/ZPlDSKcBNtk+WdBywwvbxczzGi2ljGCRR742AGPXPGxHtk4Tt9Xo3NmWc9tuA35e0Ftiv\n246IiCFa1JH2RjWQI+2IiEUbxpF2RERUltCOiGhIQjsioiEJ7YiIhiS0IyIaktCOiGhIQjsioiEJ\n7YiIhiS0IyIaktCOiGhIQjsioiEJ7YiIhiS0IyIaktCOiGhIQjsioiEJ7YiIhiS0IyIaktCOiGhI\nQjsioiEJ7YiIhiS0IyIaktCOiGhIQjsioiELhrakB0r6pqTLJF0p6cRu/wpJayStlXSepO2HX25E\nxJZNthe+k7St7TslbQV8HTgaOAS4yfYpko4DVtg+fo7HepA2hkkSUKsGMeqfNyLaJwnbmr1/oO4R\n23d2Nx8ILKck4EHA6m7/auDgJagzIiI2YKDQlrRM0mXAOuBLtr8FrLQ9DWB7HbDj8MqMiAgY/Ej7\nXtt7Ao8B9pb0ZNbvb0ifQETEkC1fzJ1t/1zSFLA/MC1ppe1pSRPADfM9btWqVffdnpycZHJycqOK\njYjYXE1NTTE1NbXg/RY8ESnpEcBdtm+TtA1wHvA2YF/gZtsn50Tk/VrLiciI2GTznYgc5Ej7UcBq\nScso3Sln2P6CpIuAMyX9CXA9cNiSVhwREesZaMjfJjWQI+2IiEXbpCF/ERExHhLaERENSWhHRDQk\noR0R0ZCEdkREQxLaETE2JiZ2RlKVj4mJnUf9426UDPlb+tYy5C9iI+W5OiND/iIiNgMJ7YiIhiS0\nIyIaktCOiGhIQjsioiEJ7YiIhlQJ7Yy7jIhYGlXGaY963GXGfka0Ic/VGRmnHRGxGUhoR0Q0JKEd\nEdGQhHZEREMS2hERDUloR0Q0JKEdEdGQhHZEREMWDG1Jj5F0oaSrJV0p6ehu/wpJayStlXSepO2H\nX25ExJZtwRmRkiaACduXS3owcAlwEHAkcJPtUyQdB6ywffwcj8+MyIgYSJ6rMzZ6RqTtdbYv727/\nArgGeAwluFd3d1sNHLx05UZExFwW1actaWfg6cBFwErb01CCHdhxqYuLiIj7Gzi0u66RfwKO6Y64\nZ7+vGN/3GRERm4nlg9xJ0nJKYH/c9ue63dOSVtqe7vq9b5j/O6zquz3ZfURERM/U1BRTU1ML3m+g\npVklfQy40faxfftOBm62fXJORC5cQ0QsLM/VGfOdiBxk9Mizga8AV1J+mwb+BrgYOBPYCbgeOMz2\nrXM8PqEdEQPJc3XGRof2EjSc0I6IgeS5OiMXQYiI2AwktCMiGpLQjohoSEI7IqIhCe2IiIYktCMi\nGpLQjohoSEI7IqIhCe2IiIYktCMiGpLQjohoSEI7IqIhCe2IiIYktCMiGpLQji3WxMTOSKryMTGx\n86h/3NhMZD3tSjXE+Mn/xfjJ32RG1tOOiNgMJLQjIhqS0I6IaEhCOyKiIQntiIiGJLQjIhqS0I6I\naMiCoS3pI5KmJV3Rt2+FpDWS1ko6T9L2wy0zIiJgsCPt04EXzNp3PHC+7d2BC4ETlrqwiIhY34Kh\nbftrwC2zdh8ErO5urwYOXuK6IiJiDhvbp72j7WkA2+uAHZeupIiImM/yJfo+C0zgX9V3e7L7iIiI\nnqmpKaampha830ALRkl6HHCu7ad229cAk7anJU0A/2Z7j3kemwWjYizl/2L85G8yY1MXjFL30fN5\n4FXd7SOAz21SdRERMZAFj7QlfYrSn7EDMA2cCHwWOAvYCbgeOMz2rfM8PkfaMZbyfzF+8jeZMd+R\ndtbTrlRDjJ/8X4yf/E1mZD3tiIjNQEI7IqIhCe2IiIYktCMiGpLQjohoSEI7IqIhCe2IiIYktCMi\nGpLQjohoSEI7IqIhCe2IiIYktCMiGpLQjohoSEI7IqIhCe2IiIYktCMiGpLQjohoSEI7IqIhCe2I\nEZuY2BlJVT4mJnYe9Y8bmyjXiKxUQ4yfcfm/GJc6xkF+FzNyjciIiM1AQjtGIl0CMa7G/X9zk7pH\nJO0PvJsS/h+xffIc90n3SKxnHP4m41DDONUxDsbhdzEONfTqWNLuEUnLgPcCLwCeDBwu6Ykb+/22\nBA9/+MTIX8Gnpqaq/szRhvxftGNTukf2Br5v+3rbdwH/CBy0NGVtnm65ZZryCj78j+np6+esIU/O\nmEv+L9qxKaH9aOAnfds/7fZFRMSQ5ETkFubtb3/3yLtoImLjbfSJSEn7AKts799tHw949snIciIy\nIiIWa64TkZsS2lsBa4H9gJ8BFwOH275mU4qMiIj5Ld/YB9q+R9JrgDXMDPlLYEdEDNHQp7FHRMTS\nyYnIiIiGJLQjYqQkbSNp91HX0YqhhLak3SQ9sLs9KeloSQ8bRlsxGEkTkg6UdICkiVHXEwEg6QDg\ncuCL3fbTJX1+tFWNt6H0aUu6HHgGsDPwBeBzwJNtv2jJG5u/hv8BvMn23d32Q4H32D6yYg0rgbcA\nv2H7hZKeBDzL9kdq1dDVcRTwRuBCQMC+wEm2P1qzjq6WRwOPo+8kuO2vVGxfwMuBXW2fJOmxwITt\niyu1fy4bWNjC9oE16uhqeQLwd8BK20+R9FTgQNv/s2INlwDPA6Zs79ntu9L2b1Vq/9gNfd32O2vU\nsRgbPXpkAffavlvSHwGn2T5N0mVDams+y4FvSjoSWElZJ+W0yjX8A3A68IZu+3vAGUDV0AZeD+xp\n+yYASTsA3wCqhrakk4GXAt8F7ul2G6gW2sD7gXspQXEScDtwNvDbldp/e/f5xcAE8Ilu+3BgulIN\nPX9P+d/4IIDtKyR9CqgW2sBdtm8rr6X3qTk64iHd590p/wO9o/wDKMOYx86wQvsuSYcDR1B+eICt\nh9TWnGyfIOl84JvALcBzbF9XswbgEbbPlHRCV9Pdku5Z6EFDcBMlnHpu7/bVdjCwu+1fjaDtnmfa\n3qt3EGH7FkkPqNW47S8DSHqH7Wf0felcSd+uVUdnW9sXzwrMuyvXcLWklwFbSfpN4GjKAUUVtt8E\nIOkrwF62b++2VwH/UquOxRjWicgjgWcBb7b9I0m7AB8fUltzkvQc4FTK0dQUcJqk36hZA3BHd1Tr\nrqZ9gNsq1wBwHeVdxypJJwIXAd+TdOxCbw+X2A+p/OI9h7u6iWG9v8kjKUfetW0nadfeRvcc2a5y\nDTdK2o2Z38VLKBPlanotZZXQXwGfBn4O/EXlGqC8G/913/avu31jp8blxlYAO9m+YqgNrd/uxcCr\nbH+3234x8Bbb1ZaPlbQXpUvmKcBVwCOBl4zgd3Hihr7eO9oYYvunUYLh0cDTgAsoT9Je+0cPs/1Z\ntbyc0kWzF7AaeAnw322fVauGro79gQ9RXshE6ef/c9vnVaxh166G36G8G/0R8ArbP65Vw7iQ9Abg\nMOCcbtfBwBm23zq6quY2rBORU8CBlO6XS4AbgK/brnZUJ2kr2/fM2rdDr1+3Yh3LKf1lAtZ2y9iO\nTPcieqsrzqqSdMSGvm57da1aAFTWfd+P8je5YFQzebsRVr2DiGtH1W0kaTtgWa9roFKbY3NCtqc7\nyPq9bvMrtmufhxvIsEL7Mtt7dqMWdrJ9oqQrbD91yRubv4beyI1H295/FCM3uqP72W4DrrR9Q4X2\n3wicafvaLiD+FXg6pd/yZbbPH3YNs+rZDvhl78W066Z4oO07K7W/FXB1zXdbG6hlW+BY4HG2/6zr\nz93d9j9XrOEe4H8BJ/RexCVdanuvCm3vu6Gv9/r+K9Tx8AXquLlGHYsxrD7t5ZIeRXm7Ue2fcJZ/\nAM4DHtVtf4/6fWV/CnyYMsTs5ZSz9ccBX5f0ygrtv5SyqBeUk8LLKF00+1Je0Gq7ANimb3sboNoL\nR/disbYb5jdqp1P6TZ/Vbf8f6o7aALia8j+xpi+81ltVbhhsf7kL5qf3bvfvq1FD5xLg293n3u1v\n990eO8MK7ZMogXmd7W91fWffH1Jb83mE7TPpTjJ147Vrj9xYDuxh+xDbhwBPorwlfCYlvIft133d\nIC8APm37nq47YFgjhzbkQbZ/0dvobm9buYYVlBELF0j6fO+jcg0Au9k+BbgLoHu3USUw+9xt+68p\nBxZflfSfqDvcDsrBxGyvqtW47V1s79p97t3ube+68HeobyhP3O6kzll92z8EDhlGWxswDiM3drLd\nP/b2hm7fzZJq9G3/StJTKON/nwu8ru9rtcMSyt9kL9uXAnQh8R+Va/jbyu3N59eStmHm/3M3+k7O\nViIA22dIuhr4FFDlXUg3JPhlwC6zXjQfAlTvkpB0NmX+xBdtj2I00cCGEtqSHkTpGngy8KDeftt/\nMoz25nEsZaD8bpK+Tjdyo2L7AFOS/pmZF7BDun3bAbdWaP8Y4J8oP/u7bP8IQNKLgFGcZDkGOEvS\n/6UExgSlC6eaWn2lAziRMnV7J0mfBJ5NxSPMzlG9G7avkvR71LvO6zcowwsfAbyjb//tQNXRVZ2/\nowxVPk3SWcDpttcu8JiRGNaJyLOAaymvpCdR+nOvsX3Mkje2ftu/DfzE9rpu5MafU8Lyu8Aba55Y\n6KZMvxj43W7XLZQpw/+tVg3jQtIyYB/gW5TRNDCC0TTdO67TgD2ABwBbAXfYfmjNOrpadqD8TgRc\nZPvGSu0+z/aF85wox/ZnatQxjiRtT5md+gbKNXD/HvjEqEd99RtWn/bjbf8t5cmwGvgDSj9uDR9k\nZpD871B++e+jBOaHKtUAlGuvUcbh3g38EaWLovrwMkk7SDpV0qWSLpH0ni4wqunecr7P9l22r+o+\nRvFEeC/lSfl9yonQoyj/H1VJOsn2Tbb/pRsxcnN3xF1Db+TGAXN8/GGNAiR9rft8u6Sf933cLunn\nNWqYo6YdKO92jqK8E30PZTz/l0ZRz3yGNo29+3xr16e6DthxSG3NtlXf0fRLgQ/ZPhs4W2Uhq6FT\nWYjn8O7jRsp6I7L93Brtz+EfKet79M4rvLyr6fmV67hA0iHAZ2qOE5/N9nV94/hP76a0n1C5jJ0k\nnWD7rd1wzDOp1GVl+8Tuc7XF0+awXVfDQxa6Yw2SzqG8A/w4cIDt3szQM0awvMAGDat75CjKIjxP\npQxtejCla+IDS97Y+m1fRRlGdLeka4FXu1tFTtJVtp9SoYZ7ga8Cf+puvRNJPxzV2ei5fm5VXEmt\nr83bKU/Wu4FfUroFXLNrQmWNiedTRkyso/Srvsr202rV0NUh4JPAlZR3YP9q+12V2j4AuML29d32\nGykv6NcDx/TOfQy5hirjwQcl6bm2/23UdQxis7vcWDcd9UWUI9zHUhaBsaTHA6ttP7tCDQcDf0w5\nufRFypHuh23vMuy256nnnZQVy87sdr0E2Nv26+Z/1OZJ0uMoo2keAPwlsD3wfldaTKybddezNaU7\n7+t0Kz/2RtYMuYYrgH1s3ynpD4F3Ut4V7gkcavsFFWr4adfunFxpSdT5+vX76hi7/v0lDW2Nydq0\n3cmmRwFrbN/R7XsC8OAaT4q+OrajnI0/nLIU6MeAc2yvqdT+7ZQhZaIc4fbGqW8F/GJEJ99WAL/J\n/UcVDX1pVkmPtf3vw25ngDo2dDRn28+rUMN3eu8sJH2UckL45G671ozIn1FGbMw5Nt1DXg+nr47T\nN/BlVx7xNpClDu2RLkw0zrqwOhR4qe39Rl3PKHTdZscAj6FcrWQf4H9XCqr7wkjS2d1kp5HoRtIc\navuMEbV/BeUk/Z2URaIOsf3t7mvftf2kCjWMVfdIS5b0ROSWHMoLsd0bvVJtBIukJ7qsOzLnk6Pm\nu47OMZSF5i+y/VyVhZtqTafvP6Ib6Uw32/dKej3lZPAovJvyovlzylDcXmDvSb2lWWvP/pyTpFfY\n/sR8vQS1egcWY1iTa1ZTTmjc2m2vAN4xjm81NnPHAq/m/pMX+t9aDf0Id5Zf2v6lJCQ9sHtBqXVB\nV89ze1TOl/Q6SnDf0dtZYx6B7Y9KOo8yous7fV9aR5lgUsO4vNvsrWE+FqNYBjHUVf4W2hfDJWlv\n4N9tr+u2j6CMEvgxsKrmRKOu/XMoofAXlBeMW4CtXeHaoSor2t1BOcLbhtI1ACMYwdLVM9cIDdcc\nYdTS1O2YMazQ/g4w2XUJ9JY//HLtIWZbOkmXAs93WevkOZRRLK+lrKK2h+3a0/r7a9uXMnLji7Z/\nvdD9Y+lJej7lRXQfylILYzt1e9hUrhz0WsrFyPsvOl19Xe+FDGtyzTuAiyT1hpgdCrx5SG3F/EY+\n0QjuW4vmvwCPp4xL/ojHZw2Qkekmnj2J+4+k+Vit9l3WUz+/b+r2+ZLGcup2BZ+lvOs4l9Fcfm5g\nw1rl72PdLKJen+mL3V32K6raStJyl2Vp96P0b/fUXJp1NWWW7FeBF1KCaujr0IyzbqTVJOV38QXK\n7+VrlGGhNevYAXgF8ErKjMxPUtbKOaKrb0vxS9unjrqIQSzpE3eOI6oPdIERo/Fp4MuSbqQsgfpV\ngG6iUc1lap/U6xqT9BHKRJ8t3Uso18u8zPaRKlda+kTNAlqaul3Be7oX0jXc//qltUdYLWipj7Zm\nH1HtwWiurByA7TdLuoCZiUa9ExjLKP13tdz3NrtbXqBi02PrP7qhf3dLeijdWuuVazh1vqnbtp9R\nuZZR+y3Ku43nMdM9YuqPsFrQUk+uubLviGo5cHEG0EffyA24/+iNkYzcGAeS3g/8DWW5g78CfgFc\nXmMRpxanbg+bpOso7wjH/qT4Uh9p54gq1mN7q1HXMG5s/9fu5gckfRF4qO1ai/8fsIGvGdjiQhu4\nCngY5R3PWFvqI+0cUUUMqDvi/V1KUH7N9jkjLmmLJWmKsirpt7h/n/bYDfnb7Fb5i2hB1z3yeMrJ\nYihDMn/gClc1anHq9rB18wbWM45DU0dxRe6IKCe49uidHO6Wfri6UtvNTd0etnEM5/kktCNG4zrK\neu/Xd9s7dfuGzvYHu89Z4K2jMbp26EIS2hEVSTqX0of9EOAaSRd328+k8vj1lqZuV/Beykies4Bn\nAP8ZeMJIK5pHQjuirrePuoA+zUzdrsHjce3QBSW0Iyqa3XfaTawZ1fOwmanbFdwp6QHA5ZJOoawr\nvmzENc0po0ciRkDSq4GTKBc4vpeZYbE1l2Z9GeXSb2M/dXvYRn3t0MVIaEeMgKTvA8+yfeMIa3gr\nZer2D+ibul3j8m/jYlyuHboY6R6JGI0fMHMhhlE5FNi1hanbQ/RZYCyuHTqohHbEaJwAfEPSN7l/\n18TRFWtoZur2EI3NtUMHldCOGI0PAhdSljAe1ciNhwHXShr7qdtDNG7XDl1Q+rQjRmAcrpna0tTt\nYRm3a4cOIqEdMQKS3kK5wPK53P8ot+rFlqM9Ce2IERiTq7E3M3U7ZqRPO2IEbO8y6hpoaOp2zBjL\nGT8RmytJf913+9BZX3tL7Xq6ySNb2b7H9unA/rVriMVJaEfU9cd9t2eva1E7MO83dVvSX5JMGHv5\nA0XUpXluz7U9bK+kZMBrKCModgLGfnLJli592hF1bWhccJVRAb2p27Z7a3n/Esja2o3I6JGIihYY\nF/wg21tXqOFS201N3Y4ZOdKOqGhMrkzf3NTtmJE+7YgtT3NTt2NGukcitjAtTt2OGQntiIiGpHsk\nIqIhCe2IiIYktCMiGpLQjohoSEI7IqIh/x+LL+/LO4IVUQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112c6ada0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.811447811448\n"
     ]
    }
   ],
   "source": [
    "#Let's do the same thing but incorporate Random Forests\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform from p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "# Plot the scores.  See how \"Pclass\", \"Sex\", \"Title\", and \"Fare\" are the best?\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "# Pick only the four best features.\n",
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Title\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=8, min_samples_leaf=4)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg,titanic[predictors],titanic[\"Survived\"],cv=3)\n",
    "\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.837261503928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:33: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "#Let use the algorithm again with the Cabin Data\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=250, max_depth=4,learning_rate=0.1), [\"Cabin\",\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]],\n",
    "    [LogisticRegression(random_state=1), [\"Cabin\",\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]],\n",
    "    [RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=4, min_samples_leaf=2),[\"Cabin\",\"Pclass\", \"Sex\", \"FamilySize\",\"Age\",\"Embarked\"]]\n",
    "]\n",
    "# Initialize the cross validation folds\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    full_test_predictions = []\n",
    "    # Make predictions for each algorithm on each fold\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm on the training data.\n",
    "        alg.fit(titanic[predictors].iloc[train,:], train_target)\n",
    "        # Select and predict on the test fold.  \n",
    "        # The .astype(float) is necessary to convert the dataframe to all floats and avoid an sklearn error.\n",
    "        test_predictions = alg.predict_proba(titanic[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_predictions.append(test_predictions)\n",
    "    # Use a simple ensembling scheme -- just average the predictions to get the final classification.\n",
    "    test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
    "    # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction.\n",
    "    test_predictions[test_predictions <= .5] = 0\n",
    "    test_predictions[test_predictions > .5] = 1\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "# Put all the predictions together into one array.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Compute accuracy by comparing to the training data.\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/pandas/core/indexing.py:115: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "titanic_test[\"Cabin\"] = titanic_test[\"Cabin\"].fillna(\"missing\")\n",
    "\n",
    "for i,y in zip(titanic_test[\"Cabin\"],titanic_test[\"Cabin\"].index):\n",
    "    if len(i) > 4:\n",
    "        titanic_test[\"Cabin\"].iloc[y] = i.split()[0]\n",
    "\n",
    "for b in cabin_class:\n",
    "    pattern = \"{}.$|{}[0-9].$|{}[0-9][0-9].$\".format(b,b,b)\n",
    "    titanic_test[\"Cabin\"] = titanic_test[\"Cabin\"].str.replace(pattern,b)\n",
    "#print(titanic_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic_test.loc[titanic_test['Cabin'] == \"missing\",'Cabin'] = 0\n",
    "titanic_test.loc[titanic_test['Cabin'] == \"A\",'Cabin'] = 1\n",
    "titanic_test.loc[titanic_test['Cabin'] == \"B\",'Cabin'] = 2\n",
    "titanic_test.loc[titanic_test['Cabin'] == \"C\",'Cabin'] = 3\n",
    "titanic_test.loc[titanic_test['Cabin'] == \"D\",'Cabin'] = 4\n",
    "titanic_test.loc[titanic_test['Cabin'] == \"E\",'Cabin'] = 5\n",
    "titanic_test.loc[titanic_test['Cabin'] == \"F\",'Cabin'] = 6\n",
    "titanic_test.loc[titanic_test['Cabin'] == \"G\",'Cabin'] = 7\n",
    "titanic_test.loc[titanic_test['Cabin'] == \"T\",'Cabin'] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = [\"Cabin\",\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=250, max_depth=4,learning_rate=0.1), [\"Cabin\",\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]],\n",
    "    [LogisticRegression(random_state=1), [\"Cabin\",\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]],\n",
    "    [RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=4, min_samples_leaf=2),[\"Cabin\",\"Pclass\", \"Sex\", \"Fare\", \"Title\",\"FamilySize\"]]\n",
    "]\n",
    "\n",
    "full_predictions = []\n",
    "for alg, predictors in algorithms:\n",
    "    # Fit the algorithm using the full training data.\n",
    "    alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "    # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error.\n",
    "    predictions = alg.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
    "    full_predictions.append(predictions)\n",
    "\n",
    "# The gradient boosting classifier generates better predictions, so we weight it higher.\n",
    "predictions = (full_predictions[0] * 3 + full_predictions[1]) / 4\n",
    "\n",
    "predictions[predictions > 0.5] = 1\n",
    "predictions[predictions <= 0.5] = 0\n",
    "predictions = predictions.astype(int)\n",
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\":titanic_test[\"PassengerId\"],\n",
    "    \"Survived\":predictions \n",
    "    })\n",
    "submission.to_csv(\"Submission_Titanic.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This improved my accuracy a lot! Let's do more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's try national origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAEsCAYAAAAIBeLrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111314278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.819304152637\n"
     ]
    }
   ],
   "source": [
    "#Let's do the same thing but incorporate Random Forests\n",
    "predictors = [\"Cabin\",\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform from p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "# Plot the scores.  See how \"Pclass\", \"Sex\", \"Title\", and \"Fare\" are the best?\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "# Pick only the four best features.\n",
    "predictors = [\"Pclass\", \"Sex\", \"Title\",\"Fare\",\"Cabin\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=12, min_samples_leaf=2)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg,titanic[predictors],titanic[\"Survived\"],cv=3)\n",
    "\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "Accuracy: 0.82 (+/- 0.00) [Logistic Regression]\n",
      "Accuracy: 0.83 (+/- 0.01) [Random Forest]\n",
      "Accuracy: 0.82 (+/- 0.01) [Gradient Boosting]\n"
     ]
    }
   ],
   "source": [
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=4, min_samples_leaf=2)\n",
    "clf3 = GradientBoostingClassifier(random_state=1, n_estimators=250, max_depth=4,learning_rate=0.1)\n",
    "\n",
    "predictors = [\"Cabin\",\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "print('5-fold cross validation:\\n')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3], ['Logistic Regression', 'Random Forest', 'Gradient Boosting']):\n",
    "\n",
    "    scores = cross_validation.cross_val_score(clf, titanic[predictors], titanic[\"Survived\"], cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "class EnsembleClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Ensemble classifier for scikit-learn estimators.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    clf : `iterable`\n",
    "      A list of scikit-learn classifier objects.\n",
    "    weights : `list` (default: `None`)\n",
    "      If `None`, the majority rule voting will be applied to the predicted class labels.\n",
    "        If a list of weights (`float` or `int`) is provided, the averaged raw probabilities (via `predict_proba`)\n",
    "        will be used to determine the most confident class label.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, clfs, weights=None):\n",
    "        self.clfs = clfs\n",
    "        self.weights = weights\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the scikit-learn estimators.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        X : numpy array, shape = [n_samples, n_features]\n",
    "            Training data\n",
    "        y : list or numpy array, shape = [n_samples]\n",
    "            Class labels\n",
    "\n",
    "        \"\"\"\n",
    "        for clf in self.clfs:\n",
    "            clf.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        X : numpy array, shape = [n_samples, n_features]\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "\n",
    "        maj : list or numpy array, shape = [n_samples]\n",
    "            Predicted class labels by majority rule\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.classes_ = np.asarray([clf.predict(X) for clf in self.clfs])\n",
    "        if self.weights:\n",
    "            avg = self.predict_proba(X)\n",
    "\n",
    "            maj = np.apply_along_axis(lambda x: max(enumerate(x), key=operator.itemgetter(1))[0], axis=1, arr=avg)\n",
    "\n",
    "        else:\n",
    "            maj = np.asarray([np.argmax(np.bincount(self.classes_[:,c])) for c in range(self.classes_.shape[1])])\n",
    "\n",
    "        return maj\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        X : numpy array, shape = [n_samples, n_features]\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "\n",
    "        avg : list or numpy array, shape = [n_samples, n_probabilities]\n",
    "            Weighted average probability for each class per sample.\n",
    "\n",
    "        \"\"\"\n",
    "        self.probas_ = [clf.predict_proba(X) for clf in self.clfs]\n",
    "        avg = np.average(self.probas_, axis=0, weights=self.weights)\n",
    "\n",
    "        return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79 (+/- 0.01) [Logistic Regression]\n",
      "Accuracy: 0.82 (+/- 0.02) [Random Forest]\n",
      "Accuracy: 0.81 (+/- 0.02) [Gradient Booster]\n",
      "Accuracy: 0.82 (+/- 0.01) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Cabin\",\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\"]\n",
    "\n",
    "np.random.seed(123)\n",
    "eclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], weights=[1,1,1])\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3,eclf], ['Logistic Regression', 'Random Forest', 'Gradient Booster', 'Ensemble']):\n",
    "\n",
    "    scores = cross_validation.cross_val_score(clf, titanic[predictors], titanic[\"Survived\"], cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79 (+/- 0.02) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf4 = SVC(probability=True)\n",
    "#clf4 = SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "#    decision_function_shape=None, degree=3, gamma='auto', kernel='sigmoid',\n",
    " #   max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "  #  tol=0.001, verbose=False)\n",
    "clf4.fit(titanic[predictors], titanic[\"Survived\"]) \n",
    "#SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    " #   decision_function_shape=None, degree=3, gamma='auto', kernel='sigmoid',\n",
    "  #  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "   # tol=0.001, verbose=False)\n",
    "scores = cross_validation.cross_val_score(clf4, titanic[predictors], titanic[\"Survived\"], cv=3, scoring='accuracy')\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SVM looks pretty terrible so let's leave it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Silvey1': 375, 'Carlsson0': 610, 'Blackwell0': 300, 'Cacic0': 405, 'Peruschitz0': 807, 'de Pelsmaeker0': 255, 'Ilieff0': 707, 'Niklasson0': 855, 'Gheorgheff0': 362, 'Spector0': 926, 'Beckwith2': 225, 'Shorney0': 92, 'Leonard0': 165, 'Stoytcheff0': 475, 'McCaffry0': 864, 'Endres0': 581, 'Mudd0': 671, 'Salander0': 852, 'Greenfield1': 94, 'de Messemaeker1': 469, 'Youseff0': 185, 'Aronsson0': 914, 'Partner0': 295, 'Buckley0': 771, 'Daly0': 432, 'Everett0': 839, 'Staneff0': 74, 'Woolner0': 55, 'Stone0': 662, 'Frost0': 412, 'Bengtsson0': 152, 'Rintamaki0': 492, 'Saalfeld0': 270, 'Wittevrongel0': 873, 'del Carlo1': 316, \"O'Keefe0\": 902, 'Vander Planke1': 19, 'Leitch0': 496, 'Burns0': 298, 'Nancarrow0': 765, 'Aldworth0': 747, 'Levy0': 264, 'Ashby0': 915, 'Hickman2': 115, 'Romaine0': 171, 'Meek0': 357, 'Eklund0': 617, 'Abelson1': 277, 'Landergren0': 328, 'Holm0': 657, 'Coleff0': 435, 'Fahlstrom0': 210, 'Nicholson0': 458, 'Crafton0': 796, 'Ridsdale0': 446, 'Chronopoulos1': 71, 'Brocklebank0': 509, 'Hirvonen2': 705, 'Troutt0': 582, \"O'Donoghue0\": 756, 'Balkic0': 688, 'McKane0': 342, 'Windelov0': 417, 'Borebank0': 803, 'Tobin0': 627, 'Bailey0': 611, 'Daher0': 714, 'de Mulder0': 258, 'Lahoud0': 443, 'Beauchamp0': 792, 'Colbert0': 919, 'Honkanen0': 198, 'Mineff0': 266, 'Arnold-Franchi1': 49, 'Davison1': 304, 'Henry0': 239, 'Moen0': 73, 'Clifford0': 408, 'Stranden0': 602, 'Chaudanson0': 733, 'Cunningham0': 355, 'Ling0': 157, 'Connolly0': 261, 'Serepeca0': 672, 'Wright0': 466, 'Mitchell0': 550, 'Mangan0': 620, 'Wirz0': 704, 'Cor0': 530, 'Birkeland0': 351, 'Hamalainen2': 224, 'Burke0': 134, 'Vestrom0': 15, 'Masselmani0': 20, 'Baclini3': 384, 'Matinoff0': 798, 'Madigan0': 181, 'Lindahl0': 223, 'Alexander0': 650, 'Elsbury0': 494, 'Giles0': 896, 'Duran y More1': 685, 'Emanuel0': 628, 'Omont0': 825, 'Coxon0': 91, 'Seward0': 383, 'Goodwin7': 59, 'Saad0': 566, 'Nicholls2': 136, 'Stankovic0': 257, 'Wiklund1': 325, 'Isham0': 163, 'Barbara1': 317, 'Crease0': 67, 'Mitkoff0': 533, 'Reeves0': 240, 'Baumann0': 156, 'Carver0': 780, 'Ahlin1': 40, 'West3': 58, 'Peters0': 557, 'Hampe0': 378, 'Davis0': 525, 'Navratil2': 138, 'Albimona0': 189, 'Charters0': 363, 'Campbell0': 401, 'Chisholm0': 860, 'Tornquist0': 245, 'Tenglin0': 762, 'Enander0': 887, 'Kalvik0': 534, 'Rheims0': 871, 'Slabenoff0': 499, 'Cook0': 546, 'Spencer1': 31, 'Denkoff0': 297, 'Rowe0': 883, 'McDermott0': 80, 'Meyer1': 34, 'McNeill0': 838, 'Peacock2': 804, 'Chapman1': 495, 'Goldsmith0': 723, 'McMahon0': 118, 'Sjoblom0': 635, \"O'Dwyer0\": 28, 'Gavey0': 511, 'Novel0': 57, 'Davies0': 336, 'Touma2': 232, 'Dahlberg0': 695, 'Johansson Palmquist0': 768, 'Karun1': 564, 'Weisz1': 125, 'Lamb0': 752, 'Newell1': 197, 'Edvardsson0': 553, 'Conlon0': 921, 'Panula5': 50, 'Louch1': 373, 'Frolicher-Stehli2': 489, 'Corn0': 147, 'Hiltunen2': 846, 'Makinen0': 763, 'Brobeck0': 782, 'Hirvonen1': 411, 'Nieminen0': 741, 'Miles0': 745, 'Coelho0': 123, 'Moussa0': 321, 'Boulos0': 497, 'Petranec0': 97, 'Cassebeer0': 809, 'Williams1': 145, 'Fortune5': 27, 'Silven2': 359, 'Millet0': 391, 'Goldenberg1': 388, 'Nysten0': 132, 'Khalil1': 753, 'Wilhelms0': 551, 'Fox0': 303, 'Knight0': 593, 'Kennedy0': 781, 'Smyth0': 805, 'Sandstrom2': 11, 'Sjostedt0': 212, 'Peduzzi0': 389, 'Jefferys2': 716, 'Sedgwick0': 302, 'Dick1': 563, 'Moraweck0': 285, 'Dean3': 90, 'Bjornstrom-Steffansson0': 371, 'Turpin1': 41, 'Astor1': 571, 'Asim0': 318, 'Brady0': 715, 'Andrews1': 249, 'Aks1': 678, 'Carter3': 340, 'Flegenheim0': 713, 'Phillips1': 818, 'Canavan0': 425, 'Betros0': 330, 'Badman0': 755, 'Fillbrook0': 892, 'Beesley0': 22, 'Petroff0': 98, 'Heikkinen0': 3, 'Bentham0': 856, 'Troupiansky0': 595, 'Vanden Steen0': 311, 'Long0': 632, 'Duff Gordon1': 467, 'Mardirosian0': 869, 'Silverthorne0': 572, 'Lundstrom0': 893, 'McNamee1': 601, 'Sobey0': 126, 'Slemen0': 652, 'Wilson0': 908, 'Johnson2': 9, 'Olsson0': 254, 'Sawyer0': 554, 'Howard1': 710, 'Gale1': 348, 'Nye0': 66, 'Finoli0': 830, 'Bird0': 801, 'Jansson0': 341, 'Baimbrigge0': 822, 'Abelseth0': 732, 'Sweet0': 841, 'Lane0': 815, 'Perkin0': 194, 'Eitemiller0': 538, 'Spinner0': 785, 'Mallet2': 656, 'MacKay0': 854, 'Minahan1': 354, 'Foo0': 529, 'Holverson1': 35, 'Lurette0': 178, 'Haas0': 265, 'Corbett0': 724, 'Christy2': 484, 'Givard0': 195, 'Watson0': 552, 'Dika0': 734, 'Banfield0': 696, 'Boulos2': 131, 'Drazenoic0': 122, 'Tomlin0': 653, 'Gill0': 683, 'Richards5': 376, 'Augustsson0': 663, 'Renouf1': 409, 'Keeping0': 744, 'Lundahl0': 522, 'Johannesen-Bratthammer0': 381, 'Ali0': 192, 'Vartanian0': 877, 'Adams0': 346, 'Milling0': 398, 'Kraeff0': 42, 'Peuchen0': 385, 'Hocking4': 625, 'Myhrman0': 626, 'Rouse0': 413, 'Stead0': 229, 'Andersson0': 137, 'Najib0': 690, 'Theobald0': 612, 'Ilett0': 82, 'Mullens0': 569, 'Simmons0': 473, 'Smart0': 402, 'Andrew0': 135, 'Gilnagh0': 146, 'Osen0': 129, 'Doling1': 95, 'Brewe0': 619, 'Ivanoff0': 597, 'Chibnall1': 155, 'Barton0': 109, 'Rothschild1': 434, 'Shelley1': 693, 'Krekorian0': 881, 'Crosby2': 455, 'Waelens0': 78, 'Norman0': 472, 'Meo0': 142, 'Cherry0': 234, 'Laitinen0': 427, 'Garside0': 481, 'Sutehall0': 697, 'Kink-Heilmann2': 168, 'Olsvigen0': 559, 'Homer0': 502, 'Soholt0': 580, 'Hart0': 353, 'Parr0': 524, 'Butler0': 544, 'Vande Walle0': 183, 'Storey0': 799, 'Abbott2': 252, 'Francatelli0': 278, 'Willer0': 772, 'Petterson1': 379, 'Klaber0': 578, 'Pearce0': 806, 'Stewart0': 64, 'Naidenoff0': 259, 'Hagland1': 386, 'Jonsson0': 477, 'Johnson0': 273, 'Strandberg0': 407, 'Guest0': 760, 'Lines1': 677, 'Roth0': 719, 'Pears1': 141, 'Hart2': 283, 'Pavlovic0': 440, 'Braund1': 1, 'Ryerson4': 280, 'Becker3': 167, 'Longley0': 518, 'Gaskell0': 637, 'Vander Cruyssen0': 689, 'Rekic0': 105, 'Allison3': 269, 'Mellinger1': 246, 'Nankoff0': 598, 'Williams0': 18, 'Nasser1': 10, 'Wilkes1': 702, 'Larsson0': 211, 'Leader0': 641, 'Eustis1': 422, 'van Billiard2': 143, 'Rice5': 17, 'Baccos0': 845, 'Jonkoff0': 609, 'Hewlett0': 16, 'Bishop1': 263, 'Downton0': 485, 'Walcroft0': 897, 'Elias2': 309, 'Hodges0': 586, 'Bucknell0': 728, 'Fry0': 654, 'Hansen0': 514, 'Meyer0': 649, 'Cleaver0': 576, 'Trout0': 344, 'Carr0': 190, 'Ayoub0': 631, 'Skoog5': 63, 'Allum0': 664, 'Mamee0': 36, 'Somerton0': 416, 'Hagardon0': 880, 'Lovell0': 209, 'Heininen0': 655, 'Alhomaki0': 670, 'Lindstrom0': 847, 'Maisner0': 399, 'Emir0': 26, 'Dowdell0': 77, 'McCormack0': 661, 'Rush0': 479, 'Loring0': 875, 'Vander Planke3': 794, 'Todoroff0': 29, 'Pinsky0': 174, 'Barry0': 754, 'Attalah0': 111, 'Foley0': 767, 'Frolicher2': 454, 'Markun0': 694, 'Sloper0': 24, 'Andrews0': 647, \"O'Connor0\": 394, 'Dorking0': 256, 'Svensson0': 424, 'Moran1': 106, 'Sivic0': 471, 'Appleton2': 478, 'Harrington0': 500, 'Minahan2': 222, 'Green0': 204, 'Ismay0': 909, 'Vovk0': 442, 'Laroche3': 43, 'Hawksford0': 599, 'Saether0': 928, 'Madill1': 562, 'Jalsevac0': 390, 'Beattie0': 778, 'Klasen2': 161, 'Patchett0': 480, 'Gillespie0': 585, 'Clark1': 851, 'Whabee0': 895, 'Lulic0': 659, 'Buss0': 337, 'Blank0': 191, 'Lindqvist1': 543, 'Rosenbaum0': 827, 'Plotcharsky0': 335, 'Reynolds0': 835, 'Mahon0': 833, 'Lemore0': 437, 'Chevre0': 726, 'Robins1': 124, 'Oxenham0': 868, 'Davies2': 460, 'Lyntakoff0': 859, 'Colley0': 542, 'Abrahim0': 706, 'Lefebre4': 162, 'Dooley0': 701, 'Ilmakangas1': 591, 'Pulbaum0': 730, 'Oliva y Ocana0': 927, 'Yousseff0': 421, 'McCrie0': 814, 'de Brito0': 890, 'Newsom2': 128, 'Pokrnic0': 863, 'Jermyn0': 322, 'Kilgannon0': 629, 'Persson1': 241, 'Hassan0': 592, 'Sincock0': 813, 'Faunthorpe1': 53, 'Sdycoff0': 352, 'Denbury0': 891, 'Smith1': 729, 'Stephenson1': 493, 'Lockyer0': 901, 'Harris1': 62, 'Pernot0': 166, 'Thomas0': 777, 'Samaan2': 48, 'Hocking3': 449, 'Dantcheff0': 639, 'Sundman0': 356, 'Hee0': 721, 'Lindblom0': 250, 'Shaughnessy0': 727, 'Robert1': 630, 'Cumings1': 2, 'Shutes0': 506, 'Smith0': 160, 'Kent0': 415, 'Stokes0': 898, 'Ware0': 903, 'Rommetvedt0': 545, 'Ross0': 486, 'Bryhl1': 590, 'Marechal0': 669, 'Barah0': 616, 'Guggenheim0': 636, 'Widener2': 329, 'Pekoniemi0': 112, 'Demetri0': 751, 'Sirota0': 667, 'Uruchurtu0': 30, 'Douglas1': 457, 'Ware1': 867, 'Myles0': 703, 'Nosworthy0': 51, 'Taussig2': 237, 'Christmann0': 87, 'Riordan0': 923, 'Montvila0': 698, 'Van der hoef0': 158, 'Nilsson0': 284, 'Ball0': 293, 'Lindeberg-Lind0': 793, 'Webber0': 117, 'Bowerman1': 312, 'Gustafsson0': 331, \"O'Brien1\": 170, 'Chapman0': 568, 'Goldschmidt0': 93, 'Sunderland0': 202, 'Howard0': 862, 'Sharp0': 462, 'Richards2': 350, 'Frauenthal2': 540, 'Bradley0': 430, 'Anderson0': 395, 'Collyer2': 216, 'Bissette0': 243, 'Greenberg0': 579, 'Razi0': 679, 'Dyker1': 757, 'Natsch1': 247, 'Gee0': 397, 'Lindell1': 503, 'Harrison0': 238, 'Bracken0': 203, 'McCrae0': 735, 'Linehan0': 843, 'Behr0': 700, 'Saade0': 865, 'Drew2': 358, 'Johnston3': 633, 'Laleff0': 691, 'LeRoy0': 452, 'Sadowitz0': 878, 'Barkworth0': 521, 'Rothes0': 613, 'Lang0': 431, 'Connaghton0': 605, 'Andersen-Jensen1': 176, 'Ibrahim Shawah0': 643, 'Badt0': 541, 'Dakic0': 560, 'Nourney0': 922, 'Payne0': 916, 'Bostandyeff0': 519, 'Tucker0': 738, 'Leyson0': 213, 'Katavelas0': 718, \"O'Driscoll0\": 47, 'Torber0': 501, 'Mernagh0': 179, 'Spedden2': 287, 'Parker0': 866, 'McGovern0': 314, 'Dibden0': 899, 'Strom2': 228, 'Robbins0': 468, 'Birnbaum0': 761, 'Paulner0': 487, 'Olsen1': 180, 'Kantor1': 96, 'Delalic0': 828, 'Olsen0': 144, 'Hippach1': 294, 'Shine0': 775, 'Petersen0': 784, 'Hanna0': 268, 'Lesurer0': 596, 'Kelly0': 271, 'Fynney0': 21, 'Chip0': 668, 'Foreman0': 387, 'Coutts2': 305, 'Caldwell2': 76, 'Marvin1': 604, 'Weir0': 567, 'Ward0': 235, 'Connors0': 113, 'Thorne0': 233, 'Farrell0': 445, 'Allen0': 5, 'Hays0': 279, 'Fleming0': 275, 'Elias0': 624, 'Lehmann0': 339, 'Stanton0': 774, 'Mulvihill0': 739, 'Lester0': 651, 'Thomson0': 834, 'Gilbert0': 917, 'Collett0': 826, 'Carrau0': 81, 'Taylor1': 547, 'Hunt0': 218, 'Quick2': 429, 'Watt0': 151, 'Calic0': 153, 'Thorneycroft1': 372, 'Culumovic0': 674, 'Kreuchen0': 884, 'Horgan0': 508, 'Lewy0': 267, 'Harbeck0': 910, 'Douglas2': 816, 'Sirayanian0': 60, 'Renouf3': 588, 'Futrelle1': 4, 'Funk0': 313, 'Herman3': 510, 'Bowen0': 515, 'Cavendish1': 600, 'Hipkins0': 912, 'Nysveen0': 292, 'Zimmerman0': 364, 'Lievens0': 622, 'Kenyon1': 392, 'Yrois0': 182, 'Matthews0': 360, 'Garfirth0': 614, 'Salkjelsvik0': 103, 'Drapkin0': 790, 'Mellors0': 208, 'Devaney0': 44, 'Wiseman0': 366, 'Stanley0': 420, 'Calderhead0': 575, 'Maguire0': 889, 'Pallas y Castello0': 907, \"O'Brien0\": 463, 'Ford4': 84, 'Mayne0': 577, 'Coleridge0': 220, 'Jacobsohn1': 199, 'Mannion0': 589, 'Ostby1': 54, 'Young0': 291, 'Molson0': 418, 'Stengel1': 766, 'Bing0': 72, 'Hegarty0': 536, 'Vendel0': 844, 'Slayter0': 290, 'Chaffee1': 89, 'Hosono0': 260, 'Jerwan0': 406, 'Moor1': 607, 'Reuchlin0': 660, 'Caram1': 482, 'Snyder1': 709, 'Pickard0': 370, 'Widegren0': 349, 'Niskanen0': 345, \"O'Connell0\": 520, 'Larsson-Rondberg0': 920, 'Lingane0': 821, 'Wheeler0': 913, 'Harknett0': 214, 'Murphy1': 219, 'Minkoff0': 740, 'Sivola0': 159, 'Morley0': 396, 'Ford0': 870, 'Graham1': 242, 'Gronnestad0': 621, 'Adahl0': 319, 'Cameron0': 193, 'Jacobsohn3': 498, 'Lundin0': 802, 'Willard0': 842, 'Andreasson0': 88, 'Padro y Manent0': 459, 'Riihivouri0': 905, 'White1': 99, 'Angle1': 439, 'Artagaveytia0': 419, 'Thayer2': 461, 'Hood0': 70, 'Barber0': 262, 'Nicola-Yarred1': 39, 'Henriksson0': 925, 'Shellard0': 423, 'Strom1': 187, 'Zabour1': 108, 'Hassab0': 558, 'Simonius-Blumer0': 531, 'Harder1': 324, 'Ovies y Rodriguez0': 742, 'Carbines0': 175, 'White0': 879, 'Rogers0': 45, 'Duquemin0': 800, 'Mack0': 623, 'Mock1': 717, 'Dennis0': 288, 'Nakid2': 333, 'Newell2': 539, 'Hansen1': 574, 'Tikkanen0': 334, 'Bateman0': 140, 'Naughton0': 924, 'Maioni0': 428, 'Ekstrom0': 121, 'Brown2': 548, 'Peltomaki0': 725, 'Nesson0': 882, 'Julian0': 900, 'Lithman0': 811, 'Mionoff0': 102, 'Dahl0': 299, 'Smiljanic0': 148, 'Holthen0': 770, 'Toomey0': 393, 'Gibson1': 906, 'Zakarian0': 788, 'Jussila0': 483, 'Hale0': 164, 'Maenpaa0': 221, 'Baxter1': 114, 'Odahl0': 307, 'Corey0': 737, 'Bjorklund0': 736, 'Asplund0': 837, 'Salonen0': 448, 'Pain0': 343, 'Bourke2': 172, 'Keefe0': 404, 'Stahelin-Maeglin0': 523, 'Sinkkonen0': 603, 'Carter1': 226, 'Moutal0': 75, \"O'Sullivan0\": 426, 'Butt0': 451, 'Angheloff0': 874, 'Franklin0': 722, 'McCoy2': 272, 'Bazzani0': 200, 'Giglio0': 130, 'Assam0': 885, 'Warren0': 861, 'Collander0': 301, 'Jenkin0': 69, 'Cornell2': 746, 'Hendekovic0': 282, 'Case0': 750, 'Midtsjo0': 857, 'Daniel0': 505, 'Leinonen0': 526, 'Cardeza1': 556, 'Karaic0': 504, 'Willey0': 532, 'Keane0': 274, 'Risien0': 453, 'Jardin0': 507, 'Daniels0': 791, 'Ohman0': 465, 'Hocking0': 840, 'Parrish1': 236, 'Hellstrom0': 810, 'Vander Planke2': 38, 'Davidson3': 759, 'Abrahamsson0': 850, 'Assaf Khalil0': 712, 'Beane1': 456, 'Turcin0': 173, 'Cohen0': 186, 'Backstrom3': 83, 'Turja0': 555, 'McGowan0': 23, 'Johanson0': 184, 'Johansson0': 100, 'Kiernan1': 196, 'Penasco y Castellana1': 276, 'Karnes0': 849, 'Thomas2': 769, 'Schabert1': 779, 'Lemberopolous0': 673, 'Toufik0': 450, 'Byles0': 139, 'Pengelly0': 217, 'Hansen2': 680, 'Pedersen0': 758, 'Aubart0': 323, 'Sadlier0': 338, 'Kassem0': 444, 'Nenkoff0': 205, 'Bowenur0': 783, 'Farthing0': 447, 'Hoyt1': 206, 'Frauenthal1': 296, 'Beavan0': 326, 'Assaf0': 711, 'Doyle0': 748, 'Slocovski0': 85, 'Lam0': 565, 'Harmer0': 634, 'Candee0': 836, 'Wick2': 286, 'Kink2': 68, 'Yasbeck1': 512, 'Icard0': 61, 'Williams-Lambert0': 308, 'Swane0': 773, 'Compton2': 665, 'Moss0': 104, 'Fischer0': 561, 'Braf0': 764, 'Mockler0': 315, 'Nasr0': 872, 'Graham0': 699, 'Rood0': 169, 'Healy0': 248, 'Goncalves0': 400, 'Van Impe2': 361, 'Reynaldo0': 380, 'McGough0': 433, 'Jarvis0': 488, 'Cotterill0': 911, 'Osman0': 642, 'Torfa0': 812, 'Lahtinen2': 281, 'Clarke1': 367, 'Dintcheff0': 787, 'Gracie0': 786, 'Botsford0': 894, 'Berriman0': 594, 'Scanlan0': 403, 'Humblen0': 570, 'Andersen0': 829, 'Porter0': 107, 'Goldsmith2': 154, 'Bidois0': 332, 'Jensen0': 527, 'Wells2': 606, 'Swift0': 682, 'Hakkarainen1': 133, 'Turkula0': 414, 'Hyman0': 848, 'Brandeis0': 808, 'Maybery0': 817, 'Walker0': 436, 'Dulles0': 888, 'Radeff0': 537, 'Nirva0': 615, 'Moran0': 6, 'Palsson4': 8, 'Kirkland0': 517, 'Geiger0': 743, 'Madsen0': 119, 'Hold1': 215, 'Head0': 832, 'Lobb1': 230, 'Pettersson0': 648, 'Perreault0': 441, 'Veal0': 819, 'Earnshaw1': 797, 'Parkes0': 251, 'Vande Velde0': 608, 'Bystrom0': 684, 'Cairns0': 244, 'Markoff0': 676, 'McEvoy0': 583, 'Thomas1': 645, 'Murdlin0': 491, 'Kallio0': 374, 'Dodge2': 382, 'Cann0': 37, 'Berglund0': 207, 'Morrow0': 470, 'Chambers1': 587, 'Reed0': 227, 'Moore0': 116, 'Jensen1': 584, 'Phillips0': 368, 'Rosenshine0': 886, 'Malachard0': 876, 'Danbom2': 365, 'Gustafsson2': 101, 'Ryan0': 438, 'Jones0': 708, 'Dimic0': 306, 'Peter2': 120, 'Pasic0': 666, 'Evans0': 776, 'Strilic0': 904, 'Yousif0': 310, 'Hogeboom1': 618, 'Kimball1': 513, 'Hilliard0': 795, 'Oreskovic0': 347, 'Murphy0': 824, 'Cribb1': 150, 'Straus1': 749, 'Kvillner0': 377, 'Lennon1': 46, 'Hoyt0': 638, 'Sage10': 149, 'Wheadon0': 33, 'Flynn0': 369, 'Doharr0': 476, 'Moubarek2': 65, 'Meanwell0': 474, 'Jussila1': 110, 'Sap0': 720, 'Andersson6': 14, 'Potter1': 692, 'Gilinski0': 490, 'Schmidt0': 789, 'Kink-Heilmann4': 918, 'Celotti0': 86, 'Ponesell0': 644, 'Hays2': 658, 'Roebling0': 686, 'Otter0': 640, 'Richard0': 127, 'Rasmussen0': 823, 'Rosblom2': 231, 'Saundercock0': 13, 'Deacon0': 831, 'Hedman0': 646, 'Glynn0': 32, 'Wenzel0': 853, 'Mangiavacchi0': 731, 'Duane0': 253, 'McCarthy0': 7, 'Sutton0': 516, 'Backstrom1': 188, 'Danoff0': 289, 'Harper1': 52, 'Gallagher0': 573, \"O'Leary0\": 535, 'Leeni0': 464, 'Salomon0': 820, 'Ringhini0': 327, 'Davidson1': 549, 'Giles1': 681, 'Sheerlinck0': 79, 'Karlsson0': 410, 'Portaluppi0': 858, 'Rugg0': 56, 'Harris0': 201, 'Warren1': 320, 'van Melkebeke0': 687, 'Abbing0': 675, 'Bonnell0': 12, 'Brown0': 177, 'Asplund6': 25, 'Sagesser0': 528}\n"
     ]
    }
   ],
   "source": [
    "print(family_id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Foreign length: 278\n",
      "Non-foreign length: 674\n",
      "Total Names: 928\n",
      "Need to classify -24\n"
     ]
    }
   ],
   "source": [
    "family_names = []\n",
    "for i in family_id_mapping.keys():\n",
    "    family_names.append(i[:-1])\n",
    "\n",
    "#print(family_names)\n",
    "foreign_name = []\n",
    "not_foreign = []\n",
    "\n",
    "suffix = [\"i\",\"o\",\"off\",\"z\",\"a\",\"ssen\",\"sson\",\"ic\",\"strom\",\"dt\",\"ic\",\"eff\",\"und\",\"heim\",\"olous\",\"sky\", \\\n",
    "         \"ian\",\"sohn\",\"linck\",\"vic\",\"ahl\",\"oulos\",\"berg\",\"nn\",\"holm\",\"qvist\",\"rek\",\"nen\",\"baum\",\"ah\",\"oog\"\\\n",
    "         ,\"vik\",\"our\",\"thal\",\"if\",\"af\",\"stad\",\"im\",\"um\",\"quist\",\"blom\",\"gren\",\"ch\",\"urt\",\"il\",\"ib\",\"gel\",\"ab\"]\n",
    "prefix = [\"Johan\",\"de\",\"van\",\"Van\",\"Dura\"]\n",
    "#for i in family_names:\n",
    "foreign_name = list(filter(lambda x:x.endswith(tuple(suffix)),family_names))\n",
    "foreign_name = foreign_name + list(filter(lambda x:x.startswith(tuple(prefix)),family_names))\n",
    "#print(foreign_name)\n",
    "print(\"\\n\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "leftover = [item for item in family_names if item not in foreign_name]\n",
    "\n",
    "suffix_not = [\"leigh\",\"ney\",\"ert\",\"er\",\"ne\",\"am\",\"wart\",\"ard\",\"is\",\"cock\",\"land\",\"ston\",\"ing\",\"ings\",\"son\",\"ton\"\\\n",
    "             ,\"ford\",\"edy\",\"hill\",\"let\",\"ll\",\"y\",\"tt\",\"child\",\"smith\",\"field\",\"ood\",\"es\",\"head\",\"man\"]\n",
    "prefix_not = [\"O'\", \"Will\",\"Mc\",\"Fitz\",\"Mac\",\"Th\"]\n",
    "not_foreign = list(filter(lambda x:x.endswith(tuple(suffix_not)),leftover))\n",
    "not_foreign = not_foreign + list(filter(lambda x:x.startswith(tuple(prefix_not)),family_names))\n",
    "#print(not_foreign)\n",
    "\n",
    "leftover = [item for item in leftover if item not in not_foreign]\n",
    "not_foreign = not_foreign +leftover\n",
    "#leftover = family_names - foreign_name\n",
    "print(\"\\n\")\n",
    "print(\"-\"*50)\n",
    "#print(leftover)\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-\"*50)\n",
    "print(\"Foreign length:\",len(foreign_name))\n",
    "print(\"Non-foreign length:\",len(not_foreign))\n",
    "print(\"Total Names:\",len(family_names))\n",
    "print(\"Need to classify\",len(family_names)-(len(foreign_name)+len(not_foreign)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>NameLength</th>\n",
       "      <th>Title</th>\n",
       "      <th>FamilyId</th>\n",
       "      <th>FamilyNames</th>\n",
       "      <th>Foreign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Braund</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>Cumings</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>Heikkinen</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>Futrelle</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Allen</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name Sex  Age  SibSp  Parch  \\\n",
       "0                            Braund, Mr. Owen Harris   0   22      1      0   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...   1   38      1      0   \n",
       "2                             Heikkinen, Miss. Laina   1   26      0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)   1   35      1      0   \n",
       "4                           Allen, Mr. William Henry   0   35      0      0   \n",
       "\n",
       "             Ticket     Fare Cabin Embarked  FamilySize  NameLength Title  \\\n",
       "0         A/5 21171   7.2500     0        0           1          23     1   \n",
       "1          PC 17599  71.2833     3        1           1          51     3   \n",
       "2  STON/O2. 3101282   7.9250     0        0           0          22     2   \n",
       "3            113803  53.1000     3        0           1          44     3   \n",
       "4            373450   8.0500     0        0           0          24     1   \n",
       "\n",
       "   FamilyId FamilyNames Foreign  \n",
       "0        -1      Braund       1  \n",
       "1        -1     Cumings       0  \n",
       "2        -1   Heikkinen       1  \n",
       "3        -1    Futrelle       0  \n",
       "4        -1       Allen       0  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now let's give each passenger an assignment: 1=foreign, 0 =not foreign\n",
    "#print((set(titanic[\"Name\"]) & set(not_foreign)))\n",
    "family_names = [row.split(\",\")[0] for row in titanic[\"Name\"]]\n",
    "#print(family_names)\n",
    "titanic[\"FamilyNames\"] = family_names\n",
    "titanic[\"Foreign\"] = titanic[\"FamilyNames\"].isin(foreign_name)\n",
    "titanic[\"Foreign\"] = titanic[\"Foreign\"].astype(str)\n",
    "titanic.loc[titanic[\"Foreign\"] == \"True\", \"Foreign\"] = 1\n",
    "titanic.loc[titanic[\"Foreign\"] == \"False\", \"Foreign\"] = 0\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's try grouping by ticket number. \n",
    "\n",
    "num_list = []\n",
    "df_nolet = pd.DataFrame(columns=titanic.columns)\n",
    "df_let = pd.DataFrame()\n",
    "df_PC = pd.DataFrame()\n",
    "df_Paris = pd.DataFrame()\n",
    "df_TON = pd.DataFrame()\n",
    "PC_list = []\n",
    "word_list = []\n",
    "\n",
    "for k in titanic.index:\n",
    "    #print(titanic.iloc[k].Ticket)\n",
    "    i = titanic.iloc[k].Ticket.split(\" \")\n",
    "    if len(i) > 1:\n",
    "        word_list.append(i[0])\n",
    "        df_let = df_let.append(titanic.iloc[k])\n",
    "        if \"PC\" in i[0]:   \n",
    "            PC_list.append(i)\n",
    "            df_PC = df_PC.append(titanic.iloc[k])\n",
    "        if \"TON\" in i[0]:\n",
    "            df_TON = df_TON.append(titanic.iloc[k])\n",
    "        if (\"Paris\" in i[0]) | (\"PARIS\" in i[0]):\n",
    "            df_Paris = df_Paris.append(titanic.iloc[k])\n",
    "    elif len(i) == 1:\n",
    "        df_nolet = df_nolet.append(titanic.iloc[k])\n",
    "\n",
    " \n",
    "#print(PC_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Age  Cabin  Embarked  FamilyId           FamilyNames  FamilySize  \\\n",
      "1     38      3         1        -1               Cumings           1   \n",
      "30    40      0         1        -1             Uruchurtu           0   \n",
      "31    28      2         1        -1               Spencer           1   \n",
      "34    28      0         1        -1                 Meyer           1   \n",
      "52    49      4         1        -1                Harper           1   \n",
      "64    28      0         1        -1               Stewart           0   \n",
      "96    71      1         1        -1           Goldschmidt           0   \n",
      "97    23      4         1        -1            Greenfield           1   \n",
      "118   24      2         1        -1                Baxter           1   \n",
      "139   24      2         1        -1                Giglio           0   \n",
      "155   51      0         1        -1              Williams           1   \n",
      "168   28      0         0        -1               Baumann           0   \n",
      "177   50      3         1        -1                 Isham           0   \n",
      "194   44      2         1        -1                 Brown           0   \n",
      "195   58      2         1        -1               Lurette           0   \n",
      "256   28      0         1        -1                Thorne           0   \n",
      "258   35      0         1        -1                  Ward           0   \n",
      "268   58      3         0        -1                Graham           1   \n",
      "269   35      3         0        -1              Bissette           0   \n",
      "273   37      3         1        -1                Natsch           1   \n",
      "295   28      0         1        -1                  Lewy           0   \n",
      "299   50      2         1        -1                Baxter           1   \n",
      "307   17      3         1        -1  Penasco y Castellana           1   \n",
      "309   30      5         1        -1           Francatelli           0   \n",
      "311   18      2         1       280               Ryerson           4   \n",
      "325   36      3         1        -1                 Young           0   \n",
      "332   38      3         0        -1                Graham           1   \n",
      "334   28      0         0        -1            Frauenthal           1   \n",
      "369   24      2         1        -1                Aubart           0   \n",
      "373   22      0         1        -1              Ringhini           0   \n",
      "375   28      0         1        -1                 Meyer           1   \n",
      "380   42      0         1        -1                Bidois           0   \n",
      "493   71      0         1        -1          Artagaveytia           0   \n",
      "505   18      3         1        -1  Penasco y Castellana           1   \n",
      "512   36      5         0        -1               McGough           0   \n",
      "513   54      0         1        -1            Rothschild           1   \n",
      "527   28      3         0        -1              Farthing           0   \n",
      "537   30      0         1        -1                 LeRoy           0   \n",
      "544   50      3         1        -1               Douglas           1   \n",
      "557   28      0         1        -1               Robbins           0   \n",
      "572   36      5         0        -1                 Flynn           0   \n",
      "599   49      1         1        -1           Duff Gordon           1   \n",
      "609   40      3         0        -1                Shutes           0   \n",
      "641   24      2         1        -1              Sagesser           0   \n",
      "645   48      4         1        -1                Harper           1   \n",
      "660   50      0         0        -1            Frauenthal           2   \n",
      "679   36      2         1        -1               Cardeza           1   \n",
      "681   27      4         1        -1                Hassab           0   \n",
      "700   18      3         1        -1                 Astor           1   \n",
      "701   35      5         0        -1          Silverthorne           0   \n",
      "707   42      5         0        -1            Calderhead           0   \n",
      "710   24      3         1        -1                 Mayne           0   \n",
      "716   38      3         1        -1                Endres           0   \n",
      "737   35      2         1        -1               Lesurer           0   \n",
      "742   21      2         1       280               Ryerson           4   \n",
      "789   46      2         1        -1            Guggenheim           0   \n",
      "793   28      0         1        -1                  Hoyt           0   \n",
      "835   39      5         1        -1               Compton           2   \n",
      "853   16      4         0        -1                 Lines           1   \n",
      "867   31      1         0        -1              Roebling           0   \n",
      "\n",
      "         Fare  Foreign                                               Name  \\\n",
      "1     71.2833        0  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
      "30    27.7208        0                           Uruchurtu, Don. Manuel E   \n",
      "31   146.5208        0     Spencer, Mrs. William Augustus (Marie Eugenie)   \n",
      "34    82.1708        0                            Meyer, Mr. Edgar Joseph   \n",
      "52    76.7292        0           Harper, Mrs. Henry Sleeper (Myna Haxtun)   \n",
      "64    27.7208        0                              Stewart, Mr. Albert A   \n",
      "96    34.6542        1                          Goldschmidt, Mr. George B   \n",
      "97    63.3583        0                    Greenfield, Mr. William Bertram   \n",
      "118  247.5208        0                           Baxter, Mr. Quigg Edmond   \n",
      "139   79.2000        1                                 Giglio, Mr. Victor   \n",
      "155   61.3792        0                        Williams, Mr. Charles Duane   \n",
      "168   25.9250        1                                Baumann, Mr. John D   \n",
      "177   28.7125        0                         Isham, Miss. Ann Elizabeth   \n",
      "194   27.7208        0          Brown, Mrs. James Joseph (Margaret Tobin)   \n",
      "195  146.5208        0                               Lurette, Miss. Elise   \n",
      "256   79.2000        0                     Thorne, Mrs. Gertrude Maybelle   \n",
      "258  512.3292        0                                   Ward, Miss. Anna   \n",
      "268  153.4625        0      Graham, Mrs. William Thompson (Edith Junkins)   \n",
      "269  135.6333        0                             Bissette, Miss. Amelia   \n",
      "273   29.7000        1                              Natsch, Mr. Charles H   \n",
      "295   27.7208        0                                  Lewy, Mr. Ervin G   \n",
      "299  247.5208        0    Baxter, Mrs. James (Helene DeLaudeniere Chaput)   \n",
      "307  108.9000        1  Penasco y Castellana, Mrs. Victor de Satode (M...   \n",
      "309   56.9292        1                     Francatelli, Miss. Laura Mabel   \n",
      "311  262.3750        0                         Ryerson, Miss. Emily Borie   \n",
      "325  135.6333        0                           Young, Miss. Marie Grice   \n",
      "332  153.4625        0                          Graham, Mr. George Edward   \n",
      "334  133.6500        1  Frauenthal, Mrs. Henry William (Clara Heinshei...   \n",
      "369   69.3000        0                      Aubart, Mme. Leontine Pauline   \n",
      "373  135.6333        1                                Ringhini, Mr. Sante   \n",
      "375   82.1708        0              Meyer, Mrs. Edgar Joseph (Leila Saks)   \n",
      "380  227.5250        0                              Bidois, Miss. Rosalie   \n",
      "493   49.5042        1                            Artagaveytia, Mr. Ramon   \n",
      "505  108.9000        1         Penasco y Castellana, Mr. Victor de Satode   \n",
      "512   26.2875        0                          McGough, Mr. James Robert   \n",
      "513   59.4000        0     Rothschild, Mrs. Martin (Elizabeth L. Barrett)   \n",
      "527  221.7792        0                                 Farthing, Mr. John   \n",
      "537  106.4250        0                                LeRoy, Miss. Bertha   \n",
      "544  106.4250        0                         Douglas, Mr. Walter Donald   \n",
      "557  227.5250        0                                Robbins, Mr. Victor   \n",
      "572   26.3875        1                   Flynn, Mr. John Irwin (\"Irving\")   \n",
      "599   56.9292        0       Duff Gordon, Sir. Cosmo Edmund (\"Mr Morgan\")   \n",
      "609  153.4625        0                          Shutes, Miss. Elizabeth W   \n",
      "641   69.3000        0                               Sagesser, Mlle. Emma   \n",
      "645   76.7292        0                          Harper, Mr. Henry Sleeper   \n",
      "660  133.6500        1                      Frauenthal, Dr. Henry William   \n",
      "679  512.3292        1                 Cardeza, Mr. Thomas Drake Martinez   \n",
      "681   76.7292        1                                 Hassab, Mr. Hammad   \n",
      "700  227.5250        0  Astor, Mrs. John Jacob (Madeleine Talmadge Force)   \n",
      "701   26.2875        0                   Silverthorne, Mr. Spencer Victor   \n",
      "707   26.2875        0                  Calderhead, Mr. Edward Pennington   \n",
      "710   49.5042        0   Mayne, Mlle. Berthe Antonine (\"Mrs de Villiers\")   \n",
      "716  227.5250        0                      Endres, Miss. Caroline Louise   \n",
      "737  512.3292        0                             Lesurer, Mr. Gustave J   \n",
      "742  262.3750        0              Ryerson, Miss. Susan Parker \"Suzette\"   \n",
      "789   79.2000        1                           Guggenheim, Mr. Benjamin   \n",
      "793   30.6958        0                           Hoyt, Mr. William Fisher   \n",
      "835   83.1583        0                        Compton, Miss. Sara Rebecca   \n",
      "853   39.4000        0                          Lines, Miss. Mary Conover   \n",
      "867   50.4958        0               Roebling, Mr. Washington Augustus II   \n",
      "\n",
      "     NameLength  Parch  PassengerId  Pclass  Sex  SibSp  Survived    Ticket  \\\n",
      "1            51      0            2       1    1      1         1  PC 17599   \n",
      "30           24      0           31       1    0      0         0  PC 17601   \n",
      "31           46      0           32       1    1      1         1  PC 17569   \n",
      "34           23      0           35       1    0      1         0  PC 17604   \n",
      "52           40      0           53       1    1      1         1  PC 17572   \n",
      "64           21      0           65       1    0      0         0  PC 17605   \n",
      "96           25      0           97       1    0      0         0  PC 17754   \n",
      "97           31      1           98       1    0      0         1  PC 17759   \n",
      "118          24      1          119       1    0      0         0  PC 17558   \n",
      "139          18      0          140       1    0      0         0  PC 17593   \n",
      "155          27      1          156       1    0      0         0  PC 17597   \n",
      "168          19      0          169       1    0      0         0  PC 17318   \n",
      "177          26      0          178       1    1      0         0  PC 17595   \n",
      "194          41      0          195       1    1      0         1  PC 17610   \n",
      "195          20      0          196       1    1      0         1  PC 17569   \n",
      "256          30      0          257       1    1      0         1  PC 17585   \n",
      "258          16      0          259       1    1      0         1  PC 17755   \n",
      "268          45      1          269       1    1      0         1  PC 17582   \n",
      "269          22      0          270       1    1      0         1  PC 17760   \n",
      "273          21      1          274       1    0      0         0  PC 17596   \n",
      "295          17      0          296       1    0      0         0  PC 17612   \n",
      "299          47      1          300       1    1      0         1  PC 17558   \n",
      "307          82      0          308       1    1      1         1  PC 17758   \n",
      "309          30      0          310       1    1      0         1  PC 17485   \n",
      "311          26      2          312       1    1      2         1  PC 17608   \n",
      "325          24      0          326       1    1      0         1  PC 17760   \n",
      "332          25      1          333       1    0      0         0  PC 17582   \n",
      "334          50      0          335       1    1      1         1  PC 17611   \n",
      "369          29      0          370       1    1      0         1  PC 17477   \n",
      "373          19      0          374       1    0      0         0  PC 17760   \n",
      "375          37      0          376       1    1      1         1  PC 17604   \n",
      "380          21      0          381       1    1      0         1  PC 17757   \n",
      "493          23      0          494       1    0      0         0  PC 17609   \n",
      "505          42      0          506       1    0      1         0  PC 17758   \n",
      "512          25      0          513       1    0      0         1  PC 17473   \n",
      "513          46      0          514       1    1      1         1  PC 17603   \n",
      "527          18      0          528       1    0      0         0  PC 17483   \n",
      "537          19      0          538       1    1      0         1  PC 17761   \n",
      "544          26      0          545       1    0      1         0  PC 17761   \n",
      "557          19      0          558       1    0      0         0  PC 17757   \n",
      "572          32      0          573       1    0      0         1  PC 17474   \n",
      "599          44      0          600       1    0      1         1  PC 17485   \n",
      "609          25      0          610       1    1      0         1  PC 17582   \n",
      "641          20      0          642       1    1      0         1  PC 17477   \n",
      "645          25      0          646       1    0      1         1  PC 17572   \n",
      "660          29      0          661       1    0      2         1  PC 17611   \n",
      "679          34      1          680       1    0      0         1  PC 17755   \n",
      "681          18      0          682       1    0      0         1  PC 17572   \n",
      "700          49      0          701       1    1      1         1  PC 17757   \n",
      "701          32      0          702       1    0      0         1  PC 17475   \n",
      "707          33      0          708       1    0      0         1  PC 17476   \n",
      "710          48      0          711       1    1      0         1  PC 17482   \n",
      "716          29      0          717       1    1      0         1  PC 17757   \n",
      "737          22      0          738       1    0      0         1  PC 17755   \n",
      "742          37      2          743       1    1      2         1  PC 17608   \n",
      "789          24      0          790       1    0      0         0  PC 17593   \n",
      "793          24      0          794       1    0      0         0  PC 17600   \n",
      "835          27      1          836       1    1      1         1  PC 17756   \n",
      "853          25      1          854       1    1      0         1  PC 17592   \n",
      "867          36      0          868       1    0      0         0  PC 17590   \n",
      "\n",
      "     Title  \n",
      "1        3  \n",
      "30       9  \n",
      "31       3  \n",
      "34       1  \n",
      "52       3  \n",
      "64       1  \n",
      "96       1  \n",
      "97       1  \n",
      "118      1  \n",
      "139      1  \n",
      "155      1  \n",
      "168      1  \n",
      "177      2  \n",
      "194      3  \n",
      "195      2  \n",
      "256      3  \n",
      "258      2  \n",
      "268      3  \n",
      "269      2  \n",
      "273      1  \n",
      "295      1  \n",
      "299      3  \n",
      "307      3  \n",
      "309      2  \n",
      "311      2  \n",
      "325      2  \n",
      "332      1  \n",
      "334      3  \n",
      "369      8  \n",
      "373      1  \n",
      "375      3  \n",
      "380      2  \n",
      "493      1  \n",
      "505      1  \n",
      "512      1  \n",
      "513      3  \n",
      "527      1  \n",
      "537      2  \n",
      "544      1  \n",
      "557      1  \n",
      "572      1  \n",
      "599      9  \n",
      "609      2  \n",
      "641      8  \n",
      "645      1  \n",
      "660      5  \n",
      "679      1  \n",
      "681      1  \n",
      "700      3  \n",
      "701      1  \n",
      "707      1  \n",
      "710      8  \n",
      "716      2  \n",
      "737      1  \n",
      "742      2  \n",
      "789      1  \n",
      "793      1  \n",
      "835      2  \n",
      "853      2  \n",
      "867      1  \n"
     ]
    }
   ],
   "source": [
    "#print(word_list)  \n",
    "print(df_PC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Survived Overall: 0.3838383838383838\n",
      "Percentage without lettering Survived: 0.38345864661654133\n",
      "Survived with Ticket lettering: 0.38495575221238937\n",
      "Survived with PC Ticket lettering: 0.65\n",
      "Survived with TON Ticket lettering: 0.2777777777777778\n",
      "Survived with Paris Ticket lettering: 0.45454545454545453\n"
     ]
    }
   ],
   "source": [
    "#df_nolet['Survived'].mean().plot(kind='bar',figsize=(10,10));\n",
    "print(\"Percentage Survived Overall:\",len(titanic[titanic[\"Survived\"] == 1])/len(titanic))\n",
    "print(\"Percentage without lettering Survived:\",len(df_nolet[df_nolet[\"Survived\"] == 1])/len(df_nolet))\n",
    "print(\"Survived with Ticket lettering:\",len(df_let[df_let[\"Survived\"] == 1])/len(df_let) )\n",
    "print(\"Survived with PC Ticket lettering:\",len(df_PC[df_PC[\"Survived\"] == 1])/len(df_PC) )\n",
    "print(\"Survived with TON Ticket lettering:\",len(df_TON[df_TON[\"Survived\"] == 1])/len(df_TON) )\n",
    "print(\"Survived with Paris Ticket lettering:\",len(df_Paris[df_Paris[\"Survived\"] == 1])/len(df_Paris) )\n",
    "\n",
    "#print(df_nolet.dtypes)\n",
    "#print(\"DataFrame with only numbers:\",df_nolet)\n",
    "#print(\"-\"*75)\n",
    "#print(\"DataFrame with lettes:\",df_let)\n",
    "#print(\"Words\")\n",
    "#print(word_list)\n",
    "#list(filter(lambda x:x.startswith(tuple(suffix_not)),leftover))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAELCAYAAAAiIMZEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nktSgvwYl3frwm4dYcgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1117a2630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic['Survived'].groupby(titanic.Foreign).mean().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.840628507295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:33: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "#Let use the algorithm again with the Cabin Data\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=250, max_depth=4,learning_rate=0.03), [\"Foreign\",\"Cabin\",\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]],\n",
    "    [LogisticRegression(random_state=1), [\"Foreign\",\"Cabin\",\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]],\n",
    "    [RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=8, min_samples_leaf=2),[\"Foreign\",\"Cabin\",\"Pclass\", \"Sex\", \"FamilySize\",\"Age\",\"Embarked\"]]\n",
    "]\n",
    "# Initialize the cross validation folds\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    full_test_predictions = []\n",
    "    # Make predictions for each algorithm on each fold\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm on the training data.\n",
    "        alg.fit(titanic[predictors].iloc[train,:], train_target)\n",
    "        # Select and predict on the test fold.  \n",
    "        # The .astype(float) is necessary to convert the dataframe to all floats and avoid an sklearn error.\n",
    "        test_predictions = alg.predict_proba(titanic[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_predictions.append(test_predictions)\n",
    "    # Use a simple ensembling scheme -- just average the predictions to get the final classification.\n",
    "    test_predictions = (full_test_predictions[0] + full_test_predictions[1] + full_test_predictions[2]) / 3\n",
    "    # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction.\n",
    "    test_predictions[test_predictions <= .5] = 0\n",
    "    test_predictions[test_predictions > .5] = 1\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "# Put all the predictions together into one array.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Compute accuracy by comparing to the training data.\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAEsCAYAAAAIBeLrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1117e2fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.766554433221\n"
     ]
    }
   ],
   "source": [
    "#Let's do the same thing but incorporate Random Forests\n",
    "predictors = [\"Foreign\",\"Cabin\",\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform from p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "# Plot the scores.  See how \"Pclass\", \"Sex\", \"Title\", and \"Fare\" are the best?\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "# Pick only the four best features.\n",
    "predictors = [\"Pclass\", \"Sex\", \"Title\",\"Cabin\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=12, min_samples_leaf=2)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg,titanic[predictors],titanic[\"Survived\"],cv=3)\n",
    "\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf1 = LogisticRegressionCV(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=6, min_samples_leaf=2)\n",
    "clf3 = GradientBoostingClassifier(random_state=1, n_estimators=250, max_depth=4,learning_rate=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79 (+/- 0.01) [Logistic Regression CV]\n",
      "Accuracy: 0.84 (+/- 0.02) [Random Forest]\n",
      "Accuracy: 0.84 (+/- 0.02) [Gradient Booster]\n",
      "Accuracy: 0.84 (+/- 0.01) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Foreign\",\"FamilyId\",\"Cabin\",\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\",\"Embarked\"]\n",
    "\n",
    "np.random.seed(123)\n",
    "eclf = EnsembleClassifier(clfs= [clf1,clf2, clf3], weights=[1,3,1])\n",
    "\n",
    "for clf, label in zip([clf1,clf2, clf3,eclf], ['Logistic Regression CV','Random Forest', 'Gradient Booster', 'Ensemble']):\n",
    "\n",
    "    scores = cross_validation.cross_val_score(clf, titanic[predictors], titanic[\"Survived\"], cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:25: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>w3</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.827178</td>\n",
       "      <td>0.022089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.827165</td>\n",
       "      <td>0.013407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.824962</td>\n",
       "      <td>0.021321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.824937</td>\n",
       "      <td>0.020570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.824911</td>\n",
       "      <td>0.013013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.824905</td>\n",
       "      <td>0.014867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.823845</td>\n",
       "      <td>0.018826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.823838</td>\n",
       "      <td>0.018563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.823801</td>\n",
       "      <td>0.017817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.823794</td>\n",
       "      <td>0.020493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.822683</td>\n",
       "      <td>0.018135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.822664</td>\n",
       "      <td>0.017249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.821566</td>\n",
       "      <td>0.012731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.820486</td>\n",
       "      <td>0.022790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.820442</td>\n",
       "      <td>0.012513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.820442</td>\n",
       "      <td>0.012513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.818252</td>\n",
       "      <td>0.022329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.818245</td>\n",
       "      <td>0.022666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.818233</td>\n",
       "      <td>0.020430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.818227</td>\n",
       "      <td>0.017482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.818195</td>\n",
       "      <td>0.011297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.817097</td>\n",
       "      <td>0.012536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.815973</td>\n",
       "      <td>0.015178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.814856</td>\n",
       "      <td>0.014954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    w1  w2  w3      mean       std\n",
       "5    1   3   1  0.827178  0.022089\n",
       "19   3   2   1  0.827165  0.013407\n",
       "7    1   3   3  0.824962  0.021321\n",
       "14   2   3   2  0.824937  0.020570\n",
       "22   3   3   1  0.824911  0.013013\n",
       "11   2   2   1  0.824905  0.014867\n",
       "3    1   2   2  0.823845  0.018826\n",
       "4    1   2   3  0.823838  0.018563\n",
       "23   3   3   2  0.823801  0.017817\n",
       "2    1   2   1  0.823794  0.020493\n",
       "6    1   3   2  0.822683  0.018135\n",
       "13   2   3   1  0.822664  0.017249\n",
       "16   3   1   1  0.821566  0.012731\n",
       "12   2   2   3  0.820486  0.022790\n",
       "17   3   1   2  0.820442  0.012513\n",
       "20   3   2   2  0.820442  0.012513\n",
       "10   2   1   3  0.818252  0.022329\n",
       "0    1   1   2  0.818245  0.022666\n",
       "1    1   1   3  0.818233  0.020430\n",
       "15   2   3   3  0.818227  0.017482\n",
       "8    2   1   1  0.818195  0.011297\n",
       "18   3   1   3  0.817097  0.012536\n",
       "21   3   2   3  0.815973  0.015178\n",
       "9    2   1   2  0.814856  0.014954"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "df = pd.DataFrame(columns=('w1', 'w2', 'w3', 'mean', 'std'))\n",
    "\n",
    "i = 0\n",
    "for w1 in range(1,4):\n",
    "    for w2 in range(1,4):\n",
    "        for w3 in range(1,4):\n",
    "\n",
    "            if len(set((w1,w2,w3))) == 1: # skip if all weights are equal\n",
    "                continue\n",
    "\n",
    "            eclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], weights=[w1,w2,w3])\n",
    "            scores = cross_validation.cross_val_score(\n",
    "                                            estimator=eclf,\n",
    "                                            X=titanic[predictors],\n",
    "                                            y=titanic[\"Survived\"],\n",
    "                                            cv=5,\n",
    "                                            scoring='accuracy',\n",
    "                                            n_jobs=1)\n",
    "\n",
    "            df.loc[i] = [w1, w2, w3, scores.mean(), scores.std()]\n",
    "            i += 1\n",
    "\n",
    "df.sort(columns=['mean', 'std'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Title</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>FamilyId</th>\n",
       "      <th>NameLength</th>\n",
       "      <th>FamilyNames</th>\n",
       "      <th>Foreign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>0</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>16</td>\n",
       "      <td>Kelly</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>1</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>32</td>\n",
       "      <td>Wilkes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>25</td>\n",
       "      <td>Myles</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>16</td>\n",
       "      <td>Wirz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>44</td>\n",
       "      <td>Hirvonen</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                          Name Sex  \\\n",
       "0          892       3                              Kelly, Mr. James   0   \n",
       "1          893       3              Wilkes, Mrs. James (Ellen Needs)   1   \n",
       "2          894       2                     Myles, Mr. Thomas Francis   0   \n",
       "3          895       3                              Wirz, Mr. Albert   0   \n",
       "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)   1   \n",
       "\n",
       "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked Title  FamilySize  \\\n",
       "0  34.5      0      0   330911   7.8292     0        2     1           0   \n",
       "1  47.0      1      0   363272   7.0000     0        0     3           1   \n",
       "2  62.0      0      0   240276   9.6875     0        2     1           0   \n",
       "3  27.0      0      0   315154   8.6625     0        0     1           0   \n",
       "4  22.0      1      1  3101298  12.2875     0        0     3           2   \n",
       "\n",
       "   FamilyId  NameLength FamilyNames Foreign  \n",
       "0        -1          16       Kelly       0  \n",
       "1        -1          32      Wilkes       0  \n",
       "2        -1          25       Myles       0  \n",
       "3        -1          16        Wirz       1  \n",
       "4        -1          44    Hirvonen       1  "
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add Foreign column to test\n",
    "family_names = [row.split(\",\")[0] for row in titanic_test[\"Name\"]]\n",
    "#print(family_names)\n",
    "titanic_test[\"FamilyNames\"] = family_names\n",
    "titanic_test[\"Foreign\"] = titanic_test[\"FamilyNames\"].isin(foreign_name)\n",
    "titanic_test[\"Foreign\"] = titanic_test[\"Foreign\"].astype(str)\n",
    "titanic_test.loc[titanic_test[\"Foreign\"] == \"True\", \"Foreign\"] = 1\n",
    "titanic_test.loc[titanic_test[\"Foreign\"] == \"False\", \"Foreign\"] = 0\n",
    "titanic_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.1291576   0.64608749  0.15481441  0.10930221  0.52586048  0.05738435\n",
      "  0.56406244  0.07716071  0.82119187  0.06625888  0.03722076  0.15385863\n",
      "  0.97304455  0.12330587  0.97612749  0.87522522  0.14481213  0.1294521\n",
      "  0.49091278  0.81582923  0.1403902   0.57648202  0.94543073  0.13642322\n",
      "  0.89852845  0.04525587  0.96426284  0.07993727  0.46749797  0.2396252\n",
      "  0.11360876  0.07623303  0.64003171  0.50524423  0.44115332  0.14494211\n",
      "  0.39720182  0.33267053  0.07363956  0.53059603  0.12139619  0.4897811\n",
      "  0.06033301  0.86811393  0.97967798  0.22196516  0.35347168  0.12854425\n",
      "  0.98597807  0.67514611  0.25770048  0.1601797   0.89236442  0.87437368\n",
      "  0.15819492  0.07257245  0.0764972   0.16262066  0.04164098  0.97549178\n",
      "  0.03722076  0.08967701  0.04778654  0.7829111   0.78502183  0.86336413\n",
      "  0.83314392  0.15734721  0.37551652  0.88050086  0.7476135   0.03722076\n",
      "  0.48486663  0.49591751  0.97218716  0.3852008   0.10912757  0.88904054\n",
      "  0.11620212  0.7476135   0.70412741  0.26664441  0.15385863  0.03722076\n",
      "  0.15747686  0.09029146  0.83314392  0.39720182  0.7476135   0.77236642\n",
      "  0.58001736  0.16997337  0.8343364   0.10912757  0.19238763  0.09037854\n",
      "  0.9754583   0.22196516  0.42591142  0.10912757  0.97233251  0.11360876\n",
      "  0.12854425  0.06153371  0.6184554   0.06241964  0.12186523  0.12854425\n",
      "  0.0571503   0.34338787  0.16835728  0.80150954  0.94038984  0.6388253\n",
      "  0.97046531  0.09029146  0.05493874  0.72804594  0.45285702  0.86634979\n",
      "  0.84571398  0.10803247  0.97604456  0.04383057  0.12854425  0.64582955\n",
      "  0.09037854  0.76570127  0.11620212  0.10912757  0.10912757  0.52662728\n",
      "  0.1653782   0.05643771  0.03722076  0.17652722  0.1294521   0.17011558\n",
      "  0.39720182  0.01871147  0.08913624  0.94783125  0.49736665  0.06652485\n",
      "  0.46481134  0.08344423  0.31375176  0.10912757  0.46481134  0.36453407\n",
      "  0.97943467  0.0895138   0.04673152  0.63272458  0.31155192  0.04475661\n",
      "  0.96751071  0.56121101  0.4897811   0.59896929  0.79176974  0.70412741\n",
      "  0.82416926  0.29121772  0.16939259  0.46739633  0.31199026  0.02888028\n",
      "  0.91811325  0.32675559  0.04475661  0.1294521   0.09398064  0.14107754\n",
      "  0.06891612  0.9141059   0.90725811  0.44263559  0.88814656  0.98199726\n",
      "  0.11620212  0.43412952  0.9768969   0.12854425  0.9735924   0.06524762\n",
      "  0.84594988  0.07552455  0.08728828  0.11620212  0.12330587  0.15385863\n",
      "  0.64470151  0.14481213  0.77781223  0.17652722  0.81388129  0.38553134\n",
      "  0.13421821  0.52478532  0.76554114  0.71409442  0.71643493  0.87682712\n",
      "  0.13421821  0.22117057  0.7476135   0.13421821  0.87943563  0.03722076\n",
      "  0.06241964  0.04209225  0.13905563  0.86634979  0.41481263  0.2477133\n",
      "  0.83314392  0.17311538  0.97304877  0.10912757  0.84907311  0.10912757\n",
      "  0.8349889   0.26526155  0.9180897   0.48299597  0.26526155  0.7476135\n",
      "  0.04828609  0.11620212  0.13693934  0.88783303  0.14701533  0.14036921\n",
      "  0.40251356  0.06033335  0.63228815  0.07993727  0.80684779  0.98108724\n",
      "  0.89891016  0.92912375  0.39542262  0.05985398  0.47481371  0.43136145\n",
      "  0.86336413  0.07203914  0.86634979  0.40127658  0.86829708  0.22196516\n",
      "  0.32046929  0.05417568  0.07363956  0.04475661  0.12854425  0.14292263\n",
      "  0.78553976  0.06153371  0.081466    0.17652722  0.855622    0.65816834\n",
      "  0.18460189  0.03722076  0.18888876  0.04475661  0.39720182  0.10930221\n",
      "  0.45285702  0.12854425  0.98084205  0.77590371  0.07993727  0.88766176\n",
      "  0.13421821  0.12330587  0.06261301  0.13421821  0.33267053  0.74332689\n",
      "  0.7476135   0.40549698  0.42779852  0.04383057  0.04383057  0.28782494\n",
      "  0.20584937  0.10912757  0.16957433  0.47911138  0.14494211  0.64105049\n",
      "  0.18150895  0.03722076  0.90296189  0.2396252   0.15558806  0.17652722\n",
      "  0.06153371  0.37986077  0.06261301  0.10930221  0.7476135   0.9101765\n",
      "  0.35355236  0.65526872  0.43537247  0.51670862  0.10930221  0.1294521\n",
      "  0.04105801  0.7476135   0.98174926  0.79176974  0.35886145  0.13421821\n",
      "  0.17652722  0.07623303  0.06153371  0.20584937  0.3544092   0.46481134\n",
      "  0.96635355  0.17833366  0.88961991  0.16895273  0.12330587  0.11620212\n",
      "  0.92947136  0.35347168  0.07993727  0.75337341  0.0764972   0.23719851\n",
      "  0.17011558  0.10272306  0.1396271   0.62002763  0.13421821  0.04105834\n",
      "  0.03050991  0.9814479   0.64470151  0.62329026  0.11620212  0.72164356\n",
      "  0.08967701  0.84182328  0.9832615   0.13421821  0.13905563  0.16213642\n",
      "  0.59391913  0.45738082  0.9646532   0.05371937  0.12854425  0.6252031\n",
      "  0.05619492  0.93854381  0.86336413  0.10930221  0.98394863  0.10834788\n",
      "  0.09370785  0.58800175  0.96695868  0.22873068  0.07368889  0.9833985\n",
      "  0.11798113  0.11620212  0.96236513  0.97779607  0.46147273  0.08439362\n",
      "  0.26134198  0.31155192  0.12854425  0.14036921  0.59353049  0.63949852\n",
      "  0.11377146  0.90870477  0.06153371  0.11620212  0.12854425  0.16491113\n",
      "  0.36952952  0.96791161  0.76741948  0.13421821  0.08915732  0.97425016\n",
      "  0.06531952  0.96022292  0.06153371  0.06475381  0.96748662  0.06261301\n",
      "  0.9568988   0.15586106  0.4842237   0.49507844  0.09255294  0.30399805\n",
      "  0.78291035  0.63137574  0.7476135   0.9619601   0.38553134  0.10912757\n",
      "  0.97060482  0.04383057  0.10912757  0.77615726]\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Foreign\",\"FamilyId\",\"Cabin\",\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\",\"Embarked\"]\n",
    "\n",
    "clf1 = LogisticRegressionCV(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=6, min_samples_leaf=2)\n",
    "clf3 = GradientBoostingClassifier(random_state=1, n_estimators=250, max_depth=4,learning_rate=0.03)\n",
    "eclf = EnsembleClassifier(clfs= [clf1,clf2, clf3], weights=[1,3,1])\n",
    "\n",
    "algorithms = [eclf]\n",
    "    \n",
    "full_predictions = []\n",
    "\n",
    "eclf.fit(titanic[predictors],titanic[\"Survived\"])\n",
    "full_predictions = eclf.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
    "#for alg, predictors in algorithms:\n",
    "#    # Fit the algorithm using the full training data.\n",
    "#    alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "    # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error.\n",
    "#    predictions = alg.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
    " #   full_predictions.append(predictions)\n",
    "\n",
    "print(full_predictions)    \n",
    "    \n",
    "# The gradient boosting classifier generates better predictions, so we weight it higher.\n",
    "#predictions = (full_predictions[0] * full_predictions[1] + full_predictions[2]) / 3\n",
    "\n",
    "predictions[predictions > 0.5] = 1\n",
    "predictions[predictions <= 0.5] = 0\n",
    "predictions = predictions.astype(int)\n",
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\":titanic_test[\"PassengerId\"],\n",
    "    \"Survived\":predictions \n",
    "    })\n",
    "submission.to_csv(\"Submission_Titanic.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's try some Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the model ...\n"
     ]
    }
   ],
   "source": [
    "print('Compiling the model ...')\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=9, output_dim=16))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(Dense(input_dim=16, output_dim=2))\n",
    "model.add(Activation(\"tanh\"))\n",
    "\n",
    "model.compile(loss='mse', optimizer=SGD(lr=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's test cross-validation on the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#score = cross_validation.cross_val_score(model,titanic[predictors], titanic[\"Survived\"], cv=3, scoring='accuracy')\n",
    "model.validation_data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " ..., \n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "#print(np.array(titanic[predictors]))\n",
    "predictors = [\"Foreign\",\"FamilyId\",\"Cabin\",\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\",\"Embarked\"]\n",
    "\n",
    "X_NN = np.array(titanic[predictors].astype(float))\n",
    "X_NN = (X_NN - X_NN.mean(axis=0)) / X_NN.std(axis=0)\n",
    "\n",
    "y = np.array(titanic[\"Survived\"])\n",
    "y_NN = np.array([[int(c == 0), int(c == 1)] for c in titanic[\"Survived\"]])\n",
    "print(y_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training, please wait ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11312847701686152"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training, please wait ...')\n",
    "\n",
    "loss = model.fit(X_NN, y_NN,validation_split=0.001, nb_epoch=500,batch_size=200, verbose=False)\n",
    "loss.history['loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ACCURACY:  0.859708193042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.89764726, -0.05696471],\n",
       "       [-0.04135371,  0.95596194],\n",
       "       [ 0.69076294,  0.28265932],\n",
       "       ..., \n",
       "       [ 0.68702209,  0.09861382],\n",
       "       [ 0.16638939,  0.76179236],\n",
       "       [ 0.87609094,  0.10232383]])"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict_classes: Of all outputs, which has the highest value?\n",
    "pred_y = model.predict(X_NN, verbose=False)\n",
    "#class_y = np.round_(pred_y, 0)\n",
    "preds  = model.predict_classes(X_NN, verbose=False)\n",
    "\n",
    "#print('\\nCONFUSION MATRIX:\\n', confusion_matrix(y_NN, preds))\n",
    "\n",
    "print('\\nACCURACY: ', accuracy_score(y, preds))\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418/418 [==============================] - 0s     \n",
      "[-0.01007506  0.62505734  0.37861666 -0.16155194  0.26413998 -0.15953688\n",
      "  0.84619188 -0.02079492  0.60699242 -0.03924857 -0.16429368  0.3558149\n",
      "  0.95451069 -0.15735683  0.94202453  0.60509837  0.38549438 -0.10758724\n",
      "  0.67565852  0.60699826 -0.01932508  0.61872071  0.94415003 -0.00959495\n",
      "  0.91999394 -0.16717634  0.93843496 -0.04315653  0.23753224  0.14515026\n",
      " -0.02760561 -0.02625982  0.49009797  0.27114856  0.71378088 -0.04312406\n",
      "  0.49072227  0.19846034 -0.20527229  0.33010912  0.05503931  0.57500672\n",
      " -0.22208442  0.75799197  0.95470601 -0.16418931  0.34197685 -0.00991236\n",
      "  0.94689459  0.73313922  0.45954889 -0.14252631  0.84785259  0.95719439\n",
      " -0.14602979 -0.04675743 -0.22162323  0.24885882 -0.15430756  0.90395457\n",
      " -0.16429368 -0.15246119 -0.1648144   0.84619987  0.91137558  0.77143264\n",
      "  0.84621233  0.30156407  0.74174106  0.95852202  0.84620202 -0.16429368\n",
      "  0.20323004  0.72555321  0.93968189 -0.00927741 -0.22006819  0.94385517\n",
      " -0.15248127  0.84620202  0.7983287  -0.04097509  0.3558149  -0.16429368\n",
      "  0.38129687  0.09141435  0.84621233  0.49072227  0.84620202  0.77772087\n",
      "  0.64672279 -0.22283897  0.89279813 -0.22006819 -0.0864438  -0.2226298\n",
      "  0.96511877 -0.16418931  0.20368685 -0.22006819  0.91077715 -0.02760561\n",
      " -0.00991236 -0.16472521  0.80904073 -0.06002696 -0.0100579  -0.00991236\n",
      " -0.21334657  0.45567995  0.06321596  0.84620434  0.89224261  0.84616119\n",
      "  0.97053236  0.09141435 -0.04927395  0.83250797  0.57976788  0.83649611\n",
      "  0.90456343  0.16692889  0.95310211 -0.22809701 -0.00991236  0.6892491\n",
      " -0.2226298   0.8229059  -0.15248127 -0.22006819 -0.22006819  0.75287533\n",
      "  0.06006055  0.06372797 -0.16429368 -0.16444226 -0.10758724 -0.12273801\n",
      "  0.49072227  0.22216612  0.33429229  0.91642106  0.58268362 -0.14611918\n",
      "  0.35435578 -0.18905166  0.45254204 -0.22006819  0.35435578  0.29581499\n",
      "  0.96017843 -0.10526615 -0.1524601   0.7228747   0.29597846 -0.22509745\n",
      "  0.92380571  0.49086416  0.57500672  0.7273035   0.84620064  0.7983287\n",
      "  0.90349376 -0.2305861   0.14137784  0.24903041  0.55807    -0.10758314\n",
      "  0.93885368  0.1965154  -0.22509745 -0.10758724 -0.171968   -0.10757287\n",
      "  0.44400248  0.9297775   0.91177326  0.44354174  0.90362114  0.94079983\n",
      " -0.15248127  0.77579445  0.95487893 -0.00991236  0.95252687 -0.1521\n",
      "  0.93093121 -0.04345559  0.2458889  -0.15248127 -0.15735683  0.3558149\n",
      "  0.41993231  0.38549438  0.81356668 -0.16444226  0.73948383  0.20419784\n",
      " -0.15232402  0.51337564  0.84618747  0.7960186   0.45355901  0.93150198\n",
      " -0.15232402  0.29338691  0.84620202 -0.15232402  0.83192474 -0.16429368\n",
      " -0.06002696 -0.16731158 -0.0018149   0.83649611  0.43217885  0.174005\n",
      "  0.84621233  0.3493534   0.95494568 -0.22006819  0.71565342 -0.22006819\n",
      "  0.82894892 -0.16465084  0.93885654  0.25360274 -0.16465084  0.84620202\n",
      " -0.14533341 -0.15248127 -0.04285743  0.95601416 -0.01402539 -0.01017768\n",
      "  0.68635935 -0.22204208  0.72349787 -0.04315653  0.69719082  0.96373826\n",
      "  0.88095766  0.89432681  0.62792909 -0.22170691  0.2538963   0.53526294\n",
      "  0.77143264 -0.02841901  0.83649611  0.39559975  0.85193986 -0.16418931\n",
      "  0.36257693 -0.20707445 -0.20527229 -0.22509745 -0.00991236 -0.16374269\n",
      "  0.90324801 -0.16472521 -0.02186859 -0.16444226  0.89111644  0.76193482\n",
      " -0.12736525 -0.16429368  0.32603812 -0.22509745  0.49072227 -0.16155194\n",
      "  0.57976788 -0.00991236  0.96571827  0.84240907 -0.04315653  0.93082225\n",
      " -0.15232402 -0.15735683 -0.1527534  -0.15232402  0.19846034  0.79506785\n",
      "  0.84620202  0.10096568 -0.03431712 -0.22809701 -0.22809701  0.29663724\n",
      "  0.02483167 -0.22006819  0.31234765  0.846129   -0.04312406  0.88095284\n",
      " -0.15855056 -0.16429368  0.82330769  0.14515026  0.28895098 -0.16444226\n",
      " -0.16472521  0.53603584 -0.1527534  -0.16155194  0.84620202  0.92803979\n",
      "  0.53915644  0.64364588  0.50707245  0.64739966 -0.16155194 -0.10758724\n",
      " -0.16543913  0.84620202  0.96416062  0.84620064  0.32572785 -0.15232402\n",
      " -0.16444226 -0.02625982 -0.16472521  0.02483167  0.65983641  0.35435578\n",
      "  0.90850806 -0.23008911  0.93541443  0.0576747  -0.15735683 -0.15248127\n",
      "  0.897497    0.34197685 -0.04315653  0.80956841 -0.22162323  0.39506376\n",
      " -0.12273801  0.23354585 -0.13957179  0.60841328 -0.15232402 -0.16542417\n",
      "  0.24080807  0.84832376  0.41993231  0.49101272 -0.15248127  0.83819377\n",
      " -0.15246119  0.9068321   0.94221538 -0.15232402 -0.0018149   0.0755301\n",
      "  0.39551875  0.71299291  0.95865768 -0.2217903  -0.00991236  0.65211535\n",
      "  0.07418479  0.91927826  0.77143264 -0.16155194  0.92315996  0.15748151\n",
      "  0.05344051 -0.00419364  0.91764456 -0.13485162 -0.14099988  0.92756802\n",
      "  0.33835647 -0.15248127  0.93011957  0.92079049  0.75087154 -0.15243784\n",
      "  0.35256952  0.29597846 -0.00991236 -0.01017768  0.60672063  0.65392721\n",
      " -0.15248345  0.86841917 -0.16472521 -0.15248127 -0.00991236  0.13150676\n",
      "  0.10744496  0.94978374  0.80365068 -0.15232402 -0.17236257  0.95777607\n",
      " -0.00888083  0.90359879 -0.16472521 -0.00987797  0.95625198 -0.1527534\n",
      "  0.89805722  0.28217626  0.6980871   0.544186   -0.13961031  0.36049911\n",
      "  0.84619957  0.7668041   0.84620202  0.93811876  0.20419784 -0.22006819\n",
      "  0.90915298 -0.22809701 -0.22006819  0.83555394]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/keras/models.py:541: UserWarning: Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n",
      "  warnings.warn('Network returning invalid probability values. '\n"
     ]
    }
   ],
   "source": [
    "#Now using the test data\n",
    "X_NN_test = np.array(titanic_test[predictors].astype(float))\n",
    "X_NN_test = (X_NN_test - X_NN_test.mean(axis=0)) / X_NN_test.std(axis=0)\n",
    "\n",
    "preds  = model.predict_classes(X_NN_test, verbose=False)\n",
    "pred = model.predict_proba(X_NN_test).astype(float)[:,1]\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5.95215918e-02   6.35835750e-01   2.66542047e-01  -2.60543632e-02\n",
      "   3.95156340e-01  -5.10055570e-02   7.05280711e-01   2.80172672e-02\n",
      "   7.14428844e-01   1.34518392e-02  -6.34662382e-02   2.54634503e-01\n",
      "   9.63824410e-01  -1.71379667e-02   9.59189547e-01   7.40187138e-01\n",
      "   2.64979348e-01   1.09749929e-02   5.83477001e-01   7.11750446e-01\n",
      "   6.01177740e-02   5.97768889e-01   9.44837196e-01   6.29983261e-02\n",
      "   9.09235630e-01  -6.09682146e-02   9.51324279e-01   1.84148554e-02\n",
      "   3.52395663e-01   1.92295602e-01   4.29173755e-02   2.48199749e-02\n",
      "   5.65094327e-01   3.88559766e-01   5.77314755e-01   5.09335143e-02\n",
      "   4.44311925e-01   2.65966538e-01  -6.57587049e-02   4.30427175e-01\n",
      "   8.81731899e-02   5.32607689e-01  -8.08185858e-02   8.13220782e-01\n",
      "   9.67295250e-01   2.89581583e-02   3.47512571e-01   5.92962634e-02\n",
      "   9.66515847e-01   7.04252433e-01   3.58564933e-01   8.76311389e-03\n",
      "   8.70170730e-01   9.15581465e-01   6.01900895e-03   1.25965726e-02\n",
      "  -7.25058761e-02   2.06438981e-01  -5.63411308e-02   9.39754407e-01\n",
      "  -6.34662382e-02  -3.14255279e-02  -5.84437602e-02   8.14709037e-01\n",
      "   8.48070119e-01   8.17564719e-01   8.39831677e-01   2.29251085e-01\n",
      "   5.58634346e-01   9.19390250e-01   7.97061308e-01  -6.34662382e-02\n",
      "   3.44449417e-01   6.10906659e-01   9.55984171e-01   1.88026642e-01\n",
      "  -5.54131226e-02   9.16474484e-01  -1.81730690e-02   7.97061308e-01\n",
      "   7.51247216e-01   1.12825849e-01   2.54634503e-01  -6.34662382e-02\n",
      "   2.69213216e-01   9.08278323e-02   8.39831677e-01   4.44311925e-01\n",
      "   7.97061308e-01   7.74938900e-01   6.13633460e-01  -2.63757035e-02\n",
      "   8.63653188e-01  -5.54131226e-02   5.27220012e-02  -6.60685294e-02\n",
      "   9.70365148e-01   2.89581583e-02   3.15200217e-01  -5.54131226e-02\n",
      "   9.41614546e-01   4.29173755e-02   5.92962634e-02  -5.15255736e-02\n",
      "   7.13764782e-01   1.25838619e-03   5.58839883e-02   5.92962634e-02\n",
      "  -7.80407324e-02   4.00165565e-01   1.15689839e-01   8.24010490e-01\n",
      "   9.16309735e-01   7.42646781e-01   9.70552531e-01   9.08278323e-02\n",
      "   2.85669704e-03   7.80912420e-01   5.16334307e-01   8.51483615e-01\n",
      "   8.75267628e-01   1.37388846e-01   9.64615700e-01  -9.20762937e-02\n",
      "   5.92962634e-02   6.67732766e-01  -6.60685294e-02   7.94148867e-01\n",
      "  -1.81730690e-02  -5.54131226e-02  -5.54131226e-02   6.39999313e-01\n",
      "   1.12648697e-01   6.00384284e-02  -6.34662382e-02   6.11268607e-03\n",
      "   1.09749929e-02   2.36818067e-02   4.44311925e-01   1.20331579e-01\n",
      "   2.11104942e-01   9.32125885e-01   5.39572278e-01  -3.98292779e-02\n",
      "   4.09381214e-01  -5.24989834e-02   3.83494525e-01  -5.54131226e-02\n",
      "   4.09381214e-01   3.30272126e-01   9.69847372e-01  -7.83341970e-03\n",
      "  -5.28049884e-02   6.77908443e-01   3.03604646e-01  -9.01133987e-02\n",
      "   9.45734150e-01   5.26387462e-01   5.32607689e-01   6.63245617e-01\n",
      "   8.19138745e-01   7.51247216e-01   8.63960361e-01   3.03726572e-02\n",
      "   1.55826934e-01   3.58371938e-01   4.34926472e-01  -3.94484567e-02\n",
      "   9.28465452e-01   2.62036605e-01  -9.01133987e-02   1.09749929e-02\n",
      "  -3.90017105e-02   1.67949023e-02   2.56369164e-01   9.21775893e-01\n",
      "   9.09644927e-01   4.42982629e-01   8.96115849e-01   9.61449856e-01\n",
      "  -1.81730690e-02   6.04879699e-01   9.65991017e-01   5.92962634e-02\n",
      "   9.63044414e-01  -4.34593749e-02   8.88430659e-01   1.59827856e-02\n",
      "   1.65863079e-01  -1.81730690e-02  -1.71379667e-02   2.54634503e-01\n",
      "   5.32386685e-01   2.64979348e-01   7.95585528e-01   6.11268607e-03\n",
      "   7.76834804e-01   2.95265669e-01  -9.08664211e-03   5.19525136e-01\n",
      "   8.06018058e-01   7.55102059e-01   5.85120973e-01   9.03992075e-01\n",
      "  -9.08664211e-03   2.56980606e-01   7.97061308e-01  -9.08664211e-03\n",
      "   8.55695203e-01  -6.34662382e-02   1.25838619e-03  -6.25397533e-02\n",
      "   6.85948222e-02   8.51483615e-01   4.23035183e-01   2.10843240e-01\n",
      "   8.39831677e-01   2.60782576e-01   9.64003439e-01  -5.54131226e-02\n",
      "   7.82312368e-01  -5.54131226e-02   8.32252399e-01   5.03755376e-02\n",
      "   9.28455052e-01   3.68362830e-01   5.03755376e-02   7.97061308e-01\n",
      "  -4.84641390e-02  -1.81730690e-02   4.70140545e-02   9.21897191e-01\n",
      "   6.65006164e-02   6.50760998e-02   5.44287054e-01  -8.07972439e-02\n",
      "   6.77861328e-01   1.84148554e-02   7.51882185e-01   9.72452048e-01\n",
      "   8.89934634e-01   9.11650143e-01   5.11610968e-01  -8.08693339e-02\n",
      "   3.64860067e-01   4.83362573e-01   8.17564719e-01   2.16430339e-02\n",
      "   8.51483615e-01   3.98651818e-01   8.59829818e-01   2.89581583e-02\n",
      "   3.41392189e-01  -7.63917825e-02  -6.57587049e-02  -9.01133987e-02\n",
      "   5.92962634e-02  -1.03397519e-02   8.44522718e-01  -5.15255736e-02\n",
      "   2.98057382e-02   6.11268607e-03   8.73291122e-01   7.10071936e-01\n",
      "   2.85546533e-02  -6.34662382e-02   2.57306934e-01  -9.01133987e-02\n",
      "   4.44311925e-01  -2.60543632e-02   5.16334307e-01   5.92962634e-02\n",
      "   9.73317369e-01   8.09165903e-01   1.84148554e-02   9.09233705e-01\n",
      "  -9.08664211e-03  -1.71379667e-02  -4.51818034e-02  -9.08664211e-03\n",
      "   2.65966538e-01   7.69242712e-01   7.97061308e-01   2.53206732e-01\n",
      "   1.96810325e-01  -9.20762937e-02  -9.20762937e-02   2.92072413e-01\n",
      "   1.16033547e-01  -5.54131226e-02   2.40756793e-01   6.62773713e-01\n",
      "   5.09335143e-02   7.60897384e-01   1.15500062e-02  -6.34662382e-02\n",
      "   8.62767858e-01   1.92295602e-01   2.22064583e-01   6.11268607e-03\n",
      "  -5.15255736e-02   4.58248563e-01  -4.51818034e-02  -2.60543632e-02\n",
      "   7.97061308e-01   9.19077705e-01   4.46239544e-01   6.49628808e-01\n",
      "   4.70934229e-01   5.82317521e-01  -2.60543632e-02   1.09749929e-02\n",
      "  -6.21204547e-02   7.97061308e-01   9.73013548e-01   8.19138745e-01\n",
      "   3.42049140e-01  -9.08664211e-03   6.11268607e-03   2.48199749e-02\n",
      "  -5.15255736e-02   1.16033547e-01   5.07784886e-01   4.09381214e-01\n",
      "   9.37437556e-01  -2.58208676e-02   9.12778396e-01   1.13007022e-01\n",
      "  -1.71379667e-02  -1.81730690e-02   9.13411871e-01   3.47512571e-01\n",
      "   1.84148554e-02   7.81489830e-01  -7.25058761e-02   3.16223138e-01\n",
      "   2.36818067e-02   1.68045417e-01  -3.81944013e-06   6.14500184e-01\n",
      "  -9.08664211e-03  -6.21128111e-02   1.35561671e-01   9.14899873e-01\n",
      "   5.32386685e-01   5.57501371e-01  -1.81730690e-02   7.80206001e-01\n",
      "  -3.14255279e-02   8.74456753e-01   9.62801756e-01  -9.08664211e-03\n",
      "   6.85948222e-02   1.18788012e-01   4.94657721e-01   5.85568835e-01\n",
      "   9.61683572e-01  -8.39783340e-02   5.92962634e-02   6.38922597e-01\n",
      "   6.49702300e-02   9.28776324e-01   8.17564719e-01  -2.60543632e-02\n",
      "   9.53632881e-01   1.32088387e-01   7.35296000e-02   2.92304240e-01\n",
      "   9.42210561e-01   4.68759032e-02  -3.37654151e-02   9.55565544e-01\n",
      "   2.28141124e-01  -1.81730690e-02   9.46163096e-01   9.49280327e-01\n",
      "   6.06191264e-01  -3.40557484e-02   3.07057823e-01   3.03604646e-01\n",
      "   5.92962634e-02   6.50760998e-02   6.00514342e-01   6.46976233e-01\n",
      "  -1.93894978e-02   8.88660852e-01  -5.15255736e-02  -1.81730690e-02\n",
      "   5.92962634e-02   1.48119697e-01   2.38487539e-01   9.58954590e-01\n",
      "   7.85582594e-01  -9.08664211e-03  -4.13294564e-02   9.66092907e-01\n",
      "   2.81996024e-02   9.31876130e-01  -5.15255736e-02   2.74182327e-02\n",
      "   9.61958632e-01  -4.51818034e-02   9.27308408e-01   2.18813534e-01\n",
      "   5.91153748e-01   5.19931645e-01  -2.36384189e-02   3.31969188e-01\n",
      "   8.14708509e-01   6.99110829e-01   7.97061308e-01   9.50054221e-01\n",
      "   2.95265669e-01  -5.54131226e-02   9.39896439e-01  -9.20762937e-02\n",
      "  -5.54131226e-02   8.05845511e-01]\n",
      "[0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0\n",
      " 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0\n",
      " 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0\n",
      " 0 0 0 0 1 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Foreign\",\"FamilyId\",\"Cabin\",\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\",\"Embarked\"]\n",
    "\n",
    "clf1 = LogisticRegressionCV(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=6, min_samples_leaf=2)\n",
    "clf3 = GradientBoostingClassifier(random_state=1, n_estimators=250, max_depth=4,learning_rate=0.03)\n",
    "eclf = EnsembleClassifier(clfs= [clf1,clf2, clf3], weights=[1,3,1])\n",
    "\n",
    "algorithms = [eclf]\n",
    "    \n",
    "full_predictions = []\n",
    "\n",
    "eclf.fit(titanic[predictors],titanic[\"Survived\"])\n",
    "full_predictions = eclf.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
    "#for alg, predictors in algorithms:\n",
    "#    # Fit the algorithm using the full training data.\n",
    "#    alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "    # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error.\n",
    "#    predictions = alg.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
    " #   full_predictions.append(predictions)\n",
    "\n",
    "print((full_predictions + pred)/2)    \n",
    "    \n",
    "# The gradient boosting classifier generates better predictions, so we weight it higher.\n",
    "#predictions = (full_predictions[0] * full_predictions[1] + full_predictions[2]) / 3\n",
    "\n",
    "predictions[predictions > 0.5] = 1\n",
    "predictions[predictions <= 0.5] = 0\n",
    "predictions = predictions.astype(int)\n",
    "print(predictions)\n",
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\":titanic_test[\"PassengerId\"],\n",
    "    \"Survived\":predictions \n",
    "    })\n",
    "submission.to_csv(\"Submission_Titanic.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Incorporate NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0 0 0\n",
      " 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1\n",
      " 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0\n",
      " 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0\n",
      " 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0\n",
      " 0 1 0 1 1 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Cabin\",\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "algorithms = [\n",
    "    #[GradientBoostingClassifier(random_state=1, n_estimators=250, max_depth=4,learning_rate=0.03), [\"Cabin\",\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]],\n",
    "    [LogisticRegression(random_state=1), [\"Cabin\",\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]],\n",
    "    [RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=4, min_samples_leaf=2),[\"Cabin\",\"Pclass\", \"Sex\", \"Fare\", \"Title\",\"FamilySize\"]]\n",
    "]\n",
    "##########################\n",
    "print('Compiling the model ...')\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=7, output_dim=12))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(Dense(input_dim=12, output_dim=2))\n",
    "model.add(Activation(\"tanh\"))\n",
    "\n",
    "model.compile(loss='mse', optimizer=SGD(lr=1))\n",
    "\n",
    "#Now using the test data\n",
    "X_NN_test = np.array(titanic_test[predictors].astype(float))\n",
    "X_NN_test = (X_NN_test - X_NN_test.mean(axis=0)) / X_NN_test.std(axis=0)\n",
    "\n",
    "preds  = model.predict_classes(X_NN_test, verbose=False)\n",
    "pred = model.predict_proba(X_NN_test).astype(float)[:,1]\n",
    "print(pred)\n",
    "############################\n",
    "full_predictions = []\n",
    "for alg, predictors in algorithms:\n",
    "    # Fit the algorithm using the full training data.\n",
    "    alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "    # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error.\n",
    "    predictions = alg.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
    "    full_predictions.append(predictions)\n",
    "\n",
    " \n",
    "# Let's see what happens when we rank random forests the highest.\n",
    "predictions = (full_predictions[0] * 3*full_predictions[1]+ 3*pred)/7 \n",
    "\n",
    "predictions[predictions > 0.5] = 1\n",
    "predictions[predictions <= 0.5] = 0\n",
    "predictions = predictions.astype(int)\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\":titanic_test[\"PassengerId\"],\n",
    "    \"Survived\":predictions \n",
    "    })\n",
    "submission.to_csv(\"Submission_Titanic.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
